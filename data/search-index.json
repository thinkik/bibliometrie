{
  "generatedAt": "2026-02-19T10:52:07.654Z",
  "entries": [
    {
      "url": "lehrgang.html",
      "title": "Lehrgang",
      "category": "Inhalte",
      "content": "Lehrgang Hier findest du die Module 1–12 des Lehrgangs. Nutze die Hinweise für deine Rolle, um die wichtigsten Punkte gezielt zu vertiefen. Der Prozess zeigt die wichtigsten Schritte von der Fragestellung bis zum Bericht. Module auswählen: Modul Modulname Kurzbeschreibung Schwierigkeit Lesezeit Start 1 Grundlagen der Bibliometrie Zentrale Begriffe, Zitationslogik und verantwortungsvoller Einsatz von Kennzahlen. Beginner 45 Minuten Start 2 Publikationsdaten und Quellen Auswahl, Abdeckung und Qualität von Datenquellen für bibliometrische Analysen. Beginner 50 Minuten Start 3 Zitationsindikatoren Zitationsmetriken verstehen, sauber berichten und im Kontext interpretieren. Beginner 55 Minuten Start 4 Metriken für Autor:innen Autor:innenmetriken einordnen und fair im Berichtskontext einsetzen. Intermediate 25 Minuten Start 5 Journalmetriken JIF, CiteScore, SNIP und SJR verantwortungsvoll vergleichen und kommunizieren. Intermediate 30 Minuten Start 6 Feld- & Zeit-Normalisierung Feld- und Zeit-Normalisierung erklären und Kennzahlen korrekt interpretieren. Intermediate 60 Minuten Start 7 Collaboration & Co-Authorship Kollaborationsindikatoren, Counting-Regeln und Netzwerke sauber anwenden. Intermediate 50 Minuten Start 8 Alternative Metriken & Aufmerksamkeit Altmetrics, Usage und Policy Mentions sinnvoll einordnen. Intermediate 40 Minuten Start 9 Datenqualität & Disambiguierung Datenqualität sichern und Autor:innen/Organisationen sauber zuordnen. Intermediate 45 Minuten Start 10 Datenquellen & Coverage Datenquellen nach Coverage, Kurationsgrad und Bias beurteilen. Intermediate 45 Minuten Start 11 Science Mapping & Wissensstrukturen Co-Citation, Coupling und Co-Word für Mapping und Visualisierung. Advanced 50 Minuten Start 12 Forschungsbewertung in der Praxis Reporting, Dashboards und Do/Don’t-Katalog für verantwortungsvolle Nutzung. Advanced 40 Minuten Start"
    },
    {
      "url": "uebungen.html",
      "title": "Übungen",
      "category": "Übungen",
      "content": "Übungen Die Übungen sind nach Leveln und Modulen strukturiert. Wähle eine Aufgabe oder Lösung direkt in der Tabelle, um die Inhalte in einem Fenster zu öffnen. Lernfortschritt Noch keine Aufgabe bearbeitet. 0% Suche in Level 1–3 Suche nach Modulnummer, Schlagworten in den Fragen oder Schwierigkeitsgrad Zurücksetzen Nutze die Infografik, um Zitationsverläufe bei den Übungen zu interpretieren. Level 1 Modul Q1 Q2 Q3 Q4 Q5 Modul 1 Aufgabe Lösung – – – – Modul 2 Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Modul 3 Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Modul 4 Aufgabe Lösung – – – – Modul 5 Aufgabe Lösung – – – – Modul 6 Aufgabe Lösung – – – – Modul 7 Aufgabe Lösung – – – – Modul 8 Aufgabe Lösung – – – – Modul 9 Aufgabe Lösung – – – – Modul 10 Aufgabe Lösung – – – – Modul 11 Aufgabe Lösung – – – – Modul 12 Aufgabe Lösung – – – – Level 2 Modul Q1 Q2 Q3 Modul 1 Aufgabe Lösung – – Modul 2 Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Modul 3 Aufgabe Lösung Aufgabe Lösung Aufgabe Lösung Modul 4 Aufgabe Lösung – – Modul 5 Aufgabe Lösung – – Modul 6 Aufgabe Lösung – – Modul 7 Aufgabe Lösung – – Modul 8 Aufgabe Lösung – – Modul 9 Aufgabe Lösung – – Modul 10 Aufgabe Lösung – – Modul 11 Aufgabe Lösung – – Modul 12 Aufgabe Lösung – – Level 3 Modul Q1 Modul 1 Aufgabe Lösung Modul 2 Aufgabe Lösung Modul 3 Aufgabe Lösung Modul 4 Aufgabe Lösung Modul 5 Aufgabe Lösung Modul 6 Aufgabe Lösung Modul 7 Aufgabe Lösung Modul 8 Aufgabe Lösung Modul 9 Aufgabe Lösung Modul 10 Aufgabe Lösung Modul 11 Aufgabe Lösung Modul 12 Aufgabe Lösung Modul 1 · Level 1 · Q1: Zentrales Konzept definieren Schließen Aufgabe: Definiere ein zentrales Konzept der Bibliometrie in zwei Sätzen. Modul 1 · Level 1 · Q1: Zentrales Konzept definieren Schließen Lösung: Beispiel: Bibliometrie untersucht wissenschaftliche Kommunikation anhand von Publikations- und Zitationsdaten und liefert Hinweise auf Sichtbarkeit und Rezeption. Sie ersetzt keine Qualitätsurteile, sondern unterstützt Kontextanalysen. Modul 2 · Level 1 · Q1: Bedeutung von „Coverage“ Schließen Aufgabe: Was bedeutet „Coverage“ in einer bibliometrischen Datenquelle? Wie viele Personen an einem Institut arbeiten. Welche Publikationsarten, Disziplinen, Sprachen und Jahre in der Quelle enthalten sind. Wie hoch der Journal Impact Factor ist. Modul 2 · Level 1 · Q1: Bedeutung von „Coverage“ Schließen Lösung: Welche Publikationsarten, Disziplinen, Sprachen und Jahre in der Quelle enthalten sind. Modul 2 · Level 1 · Q2: Responsible Metrics im Report Schließen Aufgabe: Welche Aussage ist am besten im Sinne von Responsible Metrics? Ein Report braucht keine Methodik, Hauptsache die Zahl ist klar. Zu jeder Kennzahl gehören Datenquelle, Zeitraum und Limitationen. Wenn zwei Tools verschieden zählen, nimmt man einfach den höheren Wert. Modul 2 · Level 1 · Q2: Responsible Metrics im Report Schließen Lösung: Zu jeder Kennzahl gehören Datenquelle, Zeitraum und Limitationen. Modul 2 · Level 1 · Q3: Unterschiedliche Publikationszahlen Schließen Aufgabe: Wähle die defensibelste Erklärung: Warum zeigt Quelle A mehr Publikationen als Quelle B? Quelle A hat wahrscheinlich mehr Dokumenttypen oder breitere Abdeckung indexiert. Quelle B ist automatisch falsch. Das Institut hat heimlich Publikationen gelöscht. Modul 2 · Level 1 · Q3: Unterschiedliche Publikationszahlen Schließen Lösung: Quelle A hat wahrscheinlich mehr Dokumenttypen oder breitere Abdeckung indexiert. Modul 2 · Level 1 · Q4: Vergleich von Datenquellen Schließen Aufgabe: Was ist ein sinnvoller erster Schritt, bevor du zwei Datenquellen vergleichst? Sofort ein Ranking erstellen. Dokumenttypen und Zeitraum angleichen und Coverage prüfen. Nur nach Gefühl entscheiden, welche Quelle besser ist. Modul 2 · Level 1 · Q4: Vergleich von Datenquellen Schließen Lösung: Dokumenttypen und Zeitraum angleichen und Coverage prüfen. Modul 2 · Level 1 · Q5: Pflichtangabe Datenquelle Schließen Aufgabe: Warum ist die Datenquelle in jedem Bibliometrie-Report Pflichtangabe? Weil Zahlen nur im Kontext der Coverage interpretierbar sind. Weil sonst das Layout zu leer ist. Weil Kennzahlen überall identisch sind. Modul 2 · Level 1 · Q5: Pflichtangabe Datenquelle Schließen Lösung: Weil Zahlen nur im Kontext der Coverage interpretierbar sind. Modul 3 · Level 1 · Q1: Zitationen interpretieren Schließen Aufgabe: Welche Aussage zu Zitationen ist am defensibelsten? Viele Zitationen beweisen, dass die Forschung qualitativ besser ist. Zitationen können Sichtbarkeit anzeigen, müssen aber mit Kontext interpretiert werden. Zitationen sind wertlos und sollten nie verwendet werden. Modul 3 · Level 1 · Q1: Zitationen interpretieren Schließen Lösung: Zitationen können Sichtbarkeit anzeigen, müssen aber mit Kontext interpretiert werden. Modul 3 · Level 1 · Q2: Zitationsfenster Schließen Aufgabe: Was ist ein Zitationsfenster (Citation Window)? Die Anzahl Referenzen im Literaturverzeichnis. Der Zeitraum, in dem Zitationen gezählt werden (z. B. 4 Jahre). Die Anzahl Publikationen pro Jahr. Modul 3 · Level 1 · Q2: Zitationsfenster Schließen Lösung: Der Zeitraum, in dem Zitationen gezählt werden (z. B. 4 Jahre). Modul 3 · Level 1 · Q3: Selbstzitate Schließen Aufgabe: Welche Aussage zu Selbstzitaten passt am besten? Selbstzitate sind immer Betrug. Selbstzitate können legitim sein, sollten aber je nach Zweck transparent behandelt werden. Selbstzitate zählen nie in Datenquellen. Modul 3 · Level 1 · Q3: Selbstzitate Schließen Lösung: Selbstzitate können legitim sein, sollten aber je nach Zweck transparent behandelt werden. Modul 3 · Level 1 · Q4: Zitationskontext Schließen Aufgabe: Welche Aussage ist korrekt? Alle Zitationen sind positive Anerkennung. Zitationen können auch kritisch oder neutral sein. Negative Zitationen sind unmöglich. Modul 3 · Level 1 · Q4: Zitationskontext Schließen Lösung: Zitationen können auch kritisch oder neutral sein. Modul 3 · Level 1 · Q5: Fachgebietsvergleiche Schließen Aufgabe: Warum sind Vergleiche zwischen Fachgebieten schwierig? Weil Zitations- und Publikationskulturen je nach Fach unterschiedlich sind. Weil alle Fächer identisch publizieren. Weil Datenquellen immer vollständig sind. Modul 3 · Level 1 · Q5: Fachgebietsvergleiche Schließen Lösung: Weil Zitations- und Publikationskulturen je nach Fach unterschiedlich sind. Modul 4 · Level 1 · Q1: h-Index definieren Schließen Aufgabe: Welche Definition trifft den h-Index am besten? Anzahl aller Publikationen. h Publikationen haben jeweils mindestens h Zitationen. Durchschnittliche Zitationen pro Publikation. Modul 4 · Level 1 · Q1: h-Index definieren Schließen Lösung: h Publikationen haben jeweils mindestens h Zitationen. Modul 5 · Level 1 · Q1: Journalmetriken einordnen Schließen Aufgabe: Welche Aussage ist korrekt? Journalmetriken bewerten einzelne Artikel zuverlässig. Journalmetriken sind Aggregatwerte auf Zeitschriftenebene. SNIP ist identisch mit JIF. Modul 5 · Level 1 · Q1: Journalmetriken einordnen Schließen Lösung: Journalmetriken sind Aggregatwerte auf Zeitschriftenebene. Modul 6 · Level 1 · Q1: Normalisierte Werte lesen Schließen Aufgabe: Was bedeutet ein normalisierter Wert von 1.0 (z. B. CNCI/FWCI/MNCS)? Genau 1 Zitation. Zitationen entsprechen dem Erwartungswert im Referenzset. Die Publikation ist im Top-1%. Modul 6 · Level 1 · Q1: Normalisierte Werte lesen Schließen Lösung: Zitationen entsprechen dem Erwartungswert im Referenzset. Modul 7 · Level 1 · Q1: Full vs. Fractional Counting Schließen Aufgabe: Worin besteht der wichtigste Unterschied zwischen full und fractional counting? Full counting gilt nur für Zitationen. Full counting gibt jeder Einheit volle Zählung; fractional verteilt Kredit. Fractional counting zählt nur internationale Kooperationen. Modul 7 · Level 1 · Q1: Full vs. Fractional Counting Schließen Lösung: Full counting gibt jeder Einheit volle Zählung; fractional verteilt Kredit. Modul 8 · Level 1 · Q1: Altmetrics einordnen Schließen Aufgabe: Welche Aussage ist am defensibelsten? Altmetrics messen wissenschaftliche Qualität. Altmetrics messen Aufmerksamkeit/Nutzung und müssen kontextualisiert werden. Altmetrics ersetzen Zitationsmetriken vollständig. Modul 8 · Level 1 · Q1: Altmetrics einordnen Schließen Lösung: Altmetrics messen Aufmerksamkeit/Nutzung und müssen kontextualisiert werden. Modul 9 · Level 1 · Q1: Disambiguierung richtig starten Schließen Aufgabe: Welche Reihenfolge ist als Minimalstandard am sinnvollsten? Fuzzy-Matching zuerst, dann Identifier-Join. Identifier-Join zuerst (ORCID/ROR/DOI), dann Matching für den Rest. Nur manuelle Korrektur, keine Regeln. Modul 9 · Level 1 · Q1: Disambiguierung richtig starten Schließen Lösung: Identifier-Join zuerst (ORCID/ROR/DOI), dann Matching für den Rest. Modul 10 · Level 1 · Q1: Datenquellenfamilien Schließen Aufgabe: Welche Kombination passt am besten? WoS = offen und unkuratierte Universalindexierung. OpenAlex = offen, primär aus Crossref gespeist, keine selektive Journal-Kuration. Google Scholar = stark kuratiert durch Editor:innen. Modul 10 · Level 1 · Q1: Datenquellenfamilien Schließen Lösung: OpenAlex = offen, primär aus Crossref gespeist, keine selektive Journal-Kuration. Modul 11 · Level 1 · Q1: Forschungsfront erkennen Schließen Aufgabe: Welche Methode zeigt typischerweise eher die aktuelle Forschungsfront? Co-citation. Bibliographic coupling. Co-word analysis. Modul 11 · Level 1 · Q1: Forschungsfront erkennen Schließen Lösung: Bibliographic coupling. Modul 12 · Level 1 · Q1: High-stakes erkennen Schließen Aufgabe: Welche Situation ist ein High-stakes-Assessment? Ein internes Monitoring-Dashboard ohne Konsequenzen. Hiring/Promotion mit direkten Personalentscheidungen. Ein jährlicher OA-Statusbericht. Modul 12 · Level 1 · Q1: High-stakes erkennen Schließen Lösung: Hiring/Promotion mit direkten Personalentscheidungen. Modul 1 · Level 2 · Q1: Datenset skizzieren Schließen Aufgabe: Skizziere ein kleines Datenset und schlage passende Indikatoren für eine Fragestellung vor. Modul 1 · Level 2 · Q1: Datenset skizzieren Schließen Lösung: Beispiel: Datenset mit Publikationstyp, Jahr, Zitationen und Autor:in. Indikatoren könnten Publikationsanzahl pro Jahr, Zitationen pro Publikation und Anteil verschiedener Dokumenttypen sein. Modul 2 · Level 2 · Q1: Unterschiedliche Publikationszahlen Schließen Aufgabe: Interpretation: Zwei Quellen liefern unterschiedliche Publikationszahlen. Was schreibst du in den Report (2–3 Sätze, defensiv formuliert)? Datensnippet: Einheit Zeitraum Quelle Publikationen Institut A 2021–2024 Quelle 1 120 Institut A 2021–2024 Quelle 2 155 Modul 2 · Level 2 · Q1: Unterschiedliche Publikationszahlen Schließen Lösung: Die unterschiedlichen Werte sind wahrscheinlich durch abweichende Coverage (z. B. Dokumenttypen, Journals/Proceedings, Indexierungsregeln) erklärbar. Für die Interpretation ist entscheidend, welche Datenquelle zur Fragestellung passt. Im Report werden Datenquelle, Zeitraum, einbezogene Dokumenttypen und Limitationen transparent dokumentiert. Modul 2 · Level 2 · Q2: Quelle für Monitoring-Dashboard Schließen Aufgabe: Du brauchst ein reproduzierbares Monitoring-Dashboard ohne Paywall. Welche Datenquelle passt am ehesten und welche Einschränkung nennst du? Modul 2 · Level 2 · Q2: Quelle für Monitoring-Dashboard Schließen Lösung: OpenAlex (API-basiert, offen, reproduzierbar) ist naheliegend. Einschränkung: Coverage und Metadatenqualität können je nach Fachgebiet variieren; deshalb braucht es Plausibilisierung und Dokumentation der Abfrageparameter. Modul 2 · Level 2 · Q3: Quality Check Schließen Aufgabe: Nenne drei Prüfungen, die du bei einer Publikationsliste aus einer Datenquelle machst, bevor du Kennzahlen berichtest. Modul 2 · Level 2 · Q3: Quality Check Schließen Lösung: Beispiele: Dublettenprüfung, Namensvarianten/IDs plausibilisieren, Affiliations prüfen, Dokumenttypen filtern, Zeitraum korrekt setzen. Modul 3 · Level 2 · Q1: Zitationsfenster-Effekt Schließen Aufgabe: Welche Interpretation ist korrekt? Daten (Ausschnitt): Paper Year Citations in 2024 (cum.) A 2019 45 B 2023 8 Modul 3 · Level 2 · Q1: Zitationsfenster-Effekt Schließen Lösung: Paper A hatte deutlich mehr Zeit, Zitationen zu sammeln. Ein direkter Vergleich ohne gleiches Zitationsfenster oder Altersklassen ist unfair. Für einen faireren Vergleich müsste man Zitationen innerhalb eines definierten Fensters (z. B. 2 Jahre nach Publikation) betrachten. Modul 3 · Level 2 · Q2: Selbstzitate transparent berichten Schließen Aufgabe: Formuliere eine Report-Zeile (1–2 Sätze), die Zitationen mit und ohne Selbstzitate transparent ausweist. Daten (Ausschnitt): Einheit Zeitraum Zitationen (gesamt) Zitationen (ohne Selbstzitate) Institut A 2020–2024 980 840 Modul 3 · Level 2 · Q2: Selbstzitate transparent berichten Schließen Lösung: Im Zeitraum 2020–2024 wurden in der verwendeten Datenquelle 980 Zitationen gezählt; ohne Selbstzitate sind es 840. Die Differenz kann legitime Anschlusszitationen enthalten, wird aber zur Transparenz separat ausgewiesen. Modul 3 · Level 2 · Q3: Dokumenttyp-Effekt Schließen Aufgabe: Warum kann ein Review-Artikel Zitationszahlen verzerren? Nenne zwei Gründe. Modul 3 · Level 2 · Q3: Dokumenttyp-Effekt Schließen Lösung: Reviews bündeln Literatur und werden häufig als Einstieg zitiert, wodurch sie höhere Zitationsraten haben. Zudem werden Übersichtsarbeiten fachübergreifend genutzt, was Vergleiche mit Spezialstudien verzerrt. Modul 4 · Level 2 · CALC: h-Index berechnen Schließen Aufgabe: Berechne den h-Index für die Zitationsliste (absteigend): 12, 8, 5, 3, 1. Modul 4 · Level 2 · CALC: h-Index berechnen Schließen Lösung: h = 3, weil die dritte Publikation mindestens 3 Zitationen hat, die vierte aber nur 3. Modul 5 · Level 2 · CALC: JIF berechnen Schließen Aufgabe: 480 Zitationen im Jahr 2025 auf citable items aus 2023–2024, 160 citable items. Wie groß ist der JIF (2025)? Modul 5 · Level 2 · CALC: JIF berechnen Schließen Lösung: JIF = 480 / 160 = 3,0. Modul 6 · Level 2 · CALC: NCS berechnen Schließen Aufgabe: Eine Publikation hat 15 Zitationen, der Erwartungswert ist 10. Wie groß ist NCS? Modul 6 · Level 2 · CALC: NCS berechnen Schließen Lösung: NCS = 15 / 10 = 1,5. Modul 7 · Level 2 · CALC: Kollaborationsraten Schließen Aufgabe: In 10 Publikationen sind 6 domestic, 2 international, 2 single-institution. Wie groß sind die Anteile? Modul 7 · Level 2 · CALC: Kollaborationsraten Schließen Lösung: Domestic 60%, International 20%, No-collaboration 20%. Modul 8 · Level 2 · CLASSIFY: Signaltypen zuordnen Schließen Aufgabe: Ordne die Ereignisse einem Signaltyp zu (Attention/Usage/Capture/Mention-Policy). Ein Artikel wird in einem großen Online-News-Portal besprochen. Ein Repository meldet 800 COUNTER-konforme Downloads eines Datasets. 120 Mendeley Readers speichern eine Publikation. Eine Behördenleitlinie zitiert den Artikel. Modul 8 · Level 2 · CLASSIFY: Signaltypen zuordnen Schließen Lösung: News = Attention, Downloads = Usage, Mendeley Readers = Capture, Leitlinie = Mention-Policy. Modul 9 · Level 2 · DEDUPE: Dublettenregeln Schließen Aufgabe: Nenne drei priorisierte Regeln zur Dubletten-Erkennung. Modul 9 · Level 2 · DEDUPE: Dublettenregeln Schließen Lösung: (1) DOI exakt gleich = Dublette. (2) Titel+Jahr+Erstautor:in stark ähnlich = Near-duplicate (prüfen). (3) Preprint/VoR als Versionen markieren statt löschen. Modul 10 · Level 2 · APPLY: Datenquelle für Übungen Schließen Aufgabe: Du brauchst einen Übungsdatensatz ohne Paywall. Welche Quelle wählst du und warum? Modul 10 · Level 2 · APPLY: Datenquelle für Übungen Schließen Lösung: OpenAlex (offen, API, reproduzierbar) ggf. ergänzt durch Crossref. Begründung: frei zugänglich, dokumentierte Abfragen, gut für Training und QA. Modul 11 · Level 2 · APPLY: Mapping-Methode wählen Schließen Aufgabe: Du willst die „intellektuelle Basis“ eines Feldes sichtbar machen. Welche Methode passt? Modul 11 · Level 2 · APPLY: Mapping-Methode wählen Schließen Lösung: Co-citation, weil gemeinsam zitierte Klassiker die Wissensbasis abbilden. Modul 12 · Level 2 · APPLY: Methodikbox fürs Dashboard Schließen Aufgabe: Nenne vier Pflichtangaben für eine Dashboard-Methodikbox. Modul 12 · Level 2 · APPLY: Methodikbox fürs Dashboard Schließen Lösung: Datenquelle(n), Stichtag/Exportdatum, Zeitraum/Dokumenttypen, Counting-Regel (full/fractional) sowie Coverage-Hinweise. Modul 1 · Level 3 · Q1: Interpretation mit Bias Schließen Aufgabe: Schreibe eine kurze Interpretation und benenne mögliche Bias. Modul 1 · Level 3 · Q1: Interpretation mit Bias Schließen Lösung: Beispiel: „Die Zitationszahlen sind im Beobachtungszeitraum gestiegen, was auf höhere Sichtbarkeit hindeuten kann. Mögliche Bias ergeben sich aus fachlichen Unterschieden, Dokumenttypen und unvollständiger Coverage der Datenquelle.“ Modul 2 · Level 3 · CASE: Publikationsoutput 2020–2024 Schließen Aufgabe: Definiere die Datenbasis (Quelle(n), Dokumenttypen, Zeitraum) und formuliere vier Limitationen/Warnhinweise, die du transparent kommunizierst. Modul 2 · Level 3 · CASE: Publikationsoutput 2020–2024 Schließen Lösung: Methodik-Kasten mit Quelle, Zeitraum und Dokumenttypen ergänzen. Limitationen könnten Coverage-Bias, unterschiedliche Dokumenttypen, Disambiguierung/Affiliations und Datenquellen-Vergleichbarkeit betreffen. Modul 3 · Level 3 · CASE: Zitationsreport für ein Departement Schließen Aufgabe: Formuliere einen Methodik-Kasten (Quelle, Abfragedatum, Zitationsfenster, Dokumenttypen, Umgang mit Selbstzitaten) und vier Limitationen/Warnhinweise zur Interpretation der Zitationszahlen. Modul 3 · Level 3 · CASE: Zitationsreport für ein Departement Schließen Lösung: Methodik-Kasten mit Datenquelle, Abfragedatum, Zitationsfenster, Dokumenttypen und Selbstzitate-Regel ergänzen. Limitationen können Coverage-Bias, Feldunterschiede, Zeit-/Altereffekt und Zitationskontext (kritisch/neutral) betreffen. Modul 4 · Level 3 · CASE: Autor:innenmetriken berichten Schließen Aufgabe: Erstelle einen Methodik-Kasten für h/g/i10/m und nenne vier Limitationen. Modul 4 · Level 3 · CASE: Autor:innenmetriken berichten Schließen Lösung: Methodik: Datenquelle, Stichtag, Dokumenttypen, Zitationsfenster, Umgang mit Selbstzitaten, Disambiguierung der Autor:innenprofile. Limitationen: Karrierelänge, Feldunterschiede, Datenquellen-Coverage, Dokumenttypen/Reviews. Modul 5 · Level 3 · CASE: JIF-Schwelle kritisch prüfen Schließen Aufgabe: Ein Departement will „nur Journals mit JIF>5 zählen“. Nenne vier Risiken und drei bessere Alternativen. Modul 5 · Level 3 · CASE: JIF-Schwelle kritisch prüfen Schließen Lösung: Risiken: Feldbias, Artikel-vs-Journal-Fehlschluss, Fehlanreize/Gaming, Ausschluss relevanter lokaler Forschung. Alternativen: Metrik-Set innerhalb Fachgruppen, Kombination mit Scope/Fit + Peer Review, qualitative Narrative CV. Modul 6 · Level 3 · CASE: Feld-normalisierte Vergleiche Schließen Aufgabe: Skizziere einen Methodik-Kasten (Quelle, Referenzset, Fenster, Counting) und drei Limitationen für einen MNCS/Top-10%-Vergleich. Modul 6 · Level 3 · CASE: Feld-normalisierte Vergleiche Schließen Lösung: Methodik: Quelle + Stichtag, Referenzset (Feld & Jahr), Dokumenttypen, Zitationsfenster, Counting-Regel. Limitationen: Feldabgrenzung, Output-Mix/Reviews, Coverage-Bias. Modul 7 · Level 3 · CASE: Collaboration-Report Schließen Aufgabe: Erstelle einen Methodik-Kasten und nenne vier Limitationen für einen Kollaborationsreport. Modul 7 · Level 3 · CASE: Collaboration-Report Schließen Lösung: Methodik: Quelle, Zeitraum, Dokumenttypen, Definition international/domestic, Counting-Regel, Hyperauthorship-Policy. Limitationen: Affiliation-Fehler, Feldmix, Multi-Affiliations, Konsortien/Hyperauthorship. Modul 8 · Level 3 · CASE: Altmetrics verantwortungsvoll nutzen Schließen Aufgabe: Nenne vier Risiken und drei verantwortungsvolle Einsatzarten von Altmetrics. Modul 8 · Level 3 · CASE: Altmetrics verantwortungsvoll nutzen Schließen Lösung: Risiken: Bots/Gaming, Medien-/Kontroversen-Bias, Coverage-Bias, Plattformabhängigkeit. Einsatzarten: Kommunikationsmonitoring, Impact-Storytelling mit Evidenz, COUNTER-Usage für Ressourcenplanung. Modul 9 · Level 3 · CASE: Reproduzierbarer Workflow Schließen Aufgabe: Skizziere einen Workflow (5 Schritte) und eine QA-Checkliste (5 Checks) für ein monatliches Dashboard. Modul 9 · Level 3 · CASE: Reproduzierbarer Workflow Schließen Lösung: Workflow: Datenabruf → Snapshot speichern → Cleaning/Mapping → Dedup/Disambiguierung → Kennzahlen + Versionierung. QA: DOI-Duplikate, ORCID/ROR-Quoten, Missing Affiliations, Top-Einheiten plausibilisieren, Parameter-Logging. Modul 10 · Level 3 · CASE: Source-Strategie Schließen Aufgabe: Entwirf eine Source-Strategie für ein Uni-Dashboard und eine kurze Source-Disclosure-Box. Modul 10 · Level 3 · CASE: Source-Strategie Schließen Lösung: Strategie: WoS/Scopus als Hauptquelle, OpenAlex zur Triangulation. Disclosure: Quelle, Stichtag, Query/Exportparameter, Zeitraum, Dokumenttypen, Counting, Coverage-Hinweis. Modul 11 · Level 3 · CASE: Science-Mapping dokumentieren Schließen Aufgabe: Nenne fünf Pflichtangaben für ein reproduzierbares Science-Mapping-Projekt. Modul 11 · Level 3 · CASE: Science-Mapping dokumentieren Schließen Lösung: Datenquelle, Query/Zeitraum, Dokumenttypen, Link-Typ (co-citation/coupling/co-word), Thresholds/Normalisierung sowie Tool-Version. Modul 12 · Level 3 · CASE: Bewertungs-Set entwerfen Schließen Aufgabe: Entwirf ein Bewertungs-Set für eine Berufungskommission (3 qualitative Kriterien, 3 kontextualisierte Metriken, 1 Narrative-Komponente). Modul 12 · Level 3 · CASE: Bewertungs-Set entwerfen Schließen Lösung: Qualitativ: Peer Review, Beitrag zu Team Science, Open-Science-Praktiken. Metriken: feld-normalisierte Zitationskennzahl (Aggregat), Top-10%-Anteil, Collaboration-Anteil. Narrative: Narrative CV mit Impact-Evidenz."
    },
    {
      "url": "glossar.html",
      "title": "Glossar",
      "category": "Glossar",
      "content": "Glossar Suche nach Begriffen und entdecke ihre Definitionen im Glossar. Glossar durchsuchen Tipp: Starte zu tippen, um die Liste zu filtern. Leereingabe zeigt alle Begriffe. Altmetrics Alternative Metriken, die Aufmerksamkeit in sozialen Medien, Nachrichten oder Policy-Dokumenten erfassen. Altmetrics-Kategorien Unterteilung alternativer Metriken in Gruppen wie Social Media, Nachrichten, Policy-Dokumente, wissenschaftliche Plattformen oder Nutzungsdaten. Bibliometrie Quantitative Analyse wissenschaftlicher Publikationen und ihrer Nutzung, etwa über Zitationen. Citation Verweis auf eine andere Publikation, der zeigt, wie wissenschaftliche Arbeiten miteinander verbunden sind. Citation Window Zeitfenster, in dem Zitationen für die Berechnung einer Kennzahl gezählt werden, zum Beispiel zwei oder fünf Jahre nach Publikation. Datenbereinigung Aufbereitung von Datensätzen, um Dubletten, Fehler oder Inkonsistenzen zu entfernen. g-Index Kennzahl, die häufig zitierte Arbeiten stärker gewichtet als der h-Index, indem die kumulierten Zitationen der Top-Publikationen berücksichtigt werden. h-Index Kennzahl, die Produktivität und Zitierhäufigkeit einer Person oder eines Journals kombiniert. Impact Factor Journalkennzahl, die die durchschnittliche Zitierhäufigkeit von Artikeln innerhalb eines Zeitraums beschreibt. RCR (Relative Citation Ratio) Feldnormalisierte Artikelmetrik, die die Zitierleistung einer Publikation im Verhältnis zu einem referenzierten Fachgebiet bewertet. Open Access Freier, kostenfreier Zugang zu wissenschaftlichen Publikationen, oft unter offenen Lizenzen. Peer Review Begutachtungsverfahren, bei dem Expert:innen die Qualität von Forschung vor der Veröffentlichung prüfen. SNIP (Source Normalized Impact per Paper) Journalindikator, der Zitationen kontextabhängig normalisiert und Unterschiede in Zitierpraktiken zwischen Fachgebieten ausgleicht. Keine Treffer gefunden. Bitte Suchbegriff anpassen."
    },
    {
      "url": "faq.html",
      "title": "FAQ",
      "category": "FAQ",
      "content": "FAQ Antworten auf häufige Fragen zu bibliometrischen Analysen, Kennzahlen und Datenquellen. Die Beispiele helfen dir, typische Begriffe sicher einzuordnen. Häufige Fragen zu bibliometrischen Analysen Was unterscheidet Impact Factor und H-Index? Der Impact Factor bezieht sich auf Zeitschriften und misst, wie häufig Artikel der letzten zwei Jahre im Durchschnitt zitiert wurden. Der H-Index beschreibt hingegen die Publikationsleistung einzelner Autor:innen oder Teams und kombiniert Produktivität mit Zitationen. Wie wähle ich eine geeignete Datenquelle aus? Entscheide nach Fachgebiet, Abdeckung und Datenqualität: Web of Science ist kuratiert, Scopus deckt viele Zeitschriften ab, Dimensions und OpenAlex bieten offene Daten. Prüfe außerdem, ob die Quelle deine Publikationstypen und Sprachen erfasst. Was sind Selbstzitationen? Selbstzitationen sind Verweise auf eigene Publikationen. Sie können fachlich sinnvoll sein, sollten aber transparent ausgewiesen werden, da sie Metriken künstlich erhöhen können. Warum ist Feldnormalisierung wichtig? Zitierpraktiken unterscheiden sich stark zwischen Disziplinen. Eine Feldnormalisierung erlaubt faire Vergleiche, weil Kennzahlen an fachspezifische Durchschnittswerte angepasst werden. Wie gehe ich mit fehlenden oder fehlerhaften Daten um? Antwort einblenden Prüfe Dubletten, Namensvarianten und unvollständige Metadaten. Dokumentiere Bereinigungsschritte und nutze Identifikatoren wie ORCID, um Autor:innen eindeutig zuzuordnen. Welche Rolle spielen Altmetrics? Altmetrics erfassen Aufmerksamkeit außerhalb klassischer Zitationen, etwa in sozialen Medien, Blogs oder Policy-Dokumenten. Sie ergänzen Zitationsdaten, sind aber kontextabhängig und anfällig für kurzfristige Trends."
    },
    {
      "url": "fallstudien.html",
      "title": "Fallstudien",
      "category": "Inhalte",
      "content": "Fallstudien Die Fallstudien zeigen praxisnahe Szenarien und erklären, wie bibliometrische Analysen Schritt für Schritt geplant, umgesetzt und interpretiert werden. Fallstudie 1: Resonanz auf Publikationen einer Forschungseinrichtung Eine außeruniversitäre Forschungseinrichtung möchte wissen, wie stark ihre Publikationen in den letzten fünf Jahren wahrgenommen wurden und welche Themenfelder besonders sichtbar sind. Ziel ist es, die Publikationsstrategie für die nächste Förderperiode anzupassen. Datenerhebung: Erstelle eine vollständige Liste der institutionellen Publikationen der letzten fünf Jahre inklusive DOI, Autor:innen, Affiliationen und Publikationsjahr. Wahl der Datenquelle: Entscheide dich für eine Primärquelle (z. B. Web of Science oder Scopus) und ergänze offene Quellen wie OpenAlex, um Vollständigkeit und Transparenz zu erhöhen. Bereinigung: Vereinheitliche Affiliationen, prüfe Dubletten, kläre Namensvarianten und ergänze fehlende DOIs oder ORCID-IDs. Analyse: Berechne Zitationskennzahlen (z. B. Zitationen pro Publikation), führe eine Themencluster-Analyse der Abstracts durch und vergleiche die Sichtbarkeit nach Fachbereichen. Interpretation: Ordne die Ergebnisse in den Kontext der Fachkulturen ein, berücksichtige Publikationsfenster und prüfe, ob Spitzenwerte auf einzelne Arbeiten oder langfristige Trends zurückgehen. Wichtigste Erkenntnisse Die Einrichtung erkennt, dass interdisziplinäre Projekte überdurchschnittlich zitiert werden, während bestimmte Kernthemen noch wenig Sichtbarkeit erzielen. Daraus lässt sich ableiten, gezielt Kooperationen zu fördern und die Sichtbarkeit schwächerer Themenfelder zu stärken. Passende Module: Modul 1 (Grundlagen), Modul 2 (Datenquellen), Modul 3 (Datenbereinigung) Fallstudie 2: Bibliometrische Unterstützung für eine Fördermittelanfrage Ein Fachbereich plant eine große Fördermittelanfrage und benötigt eine bibliometrische Begründung, die die eigene Leistungsfähigkeit sowie die internationale Einbettung belegt. Datenerhebung: Sammle Publikationsdaten der beteiligten Forschenden, inklusive Koautor:innen, Journals, Konferenzen und Zitationsdaten. Wahl der Datenquelle: Kombiniere interne Publikationslisten mit einer bibliometrischen Datenbank (z. B. Dimensions) und verifiziere die Zuordnung über ORCID. Bereinigung: Entferne Dubletten, vereinheitliche Projektnamen, und prüfe, ob Publikationen außerhalb der Förderperiode markiert werden. Analyse: Erstelle Kennzahlen zu Produktivität, Impact und Kollaboration, visualisiere Co-Autor:innen-Netzwerke und analysiere internationale Kooperationen. Interpretation: Vergleiche die Ergebnisse mit Benchmarks ähnlicher Fachbereiche und identifiziere Stärken, die die Förderziele unterstützen. Wichtigste Erkenntnisse Die Analyse belegt eine starke internationale Vernetzung und eine hohe Publikationsdichte in priorisierten Themenfeldern. Damit kann der Fachbereich seine Erfolgschancen im Förderantrag untermauern und gezielt argumentieren. Passende Module: Modul 2 (Datenquellen), Modul 5 (Netzwerke & Kollaboration) Fallstudie 3: Autor:innenmetriken für Karriere- und Fördergespräche Ein Fakultätsreferat soll für anstehende Karrieregespräche transparente Kennzahlen zur Publikationsleistung liefern. Fokus sind h-Index, g-Index und m-Quotient, inklusive nachvollziehbarer Datenquellen und Limitationen. Datenerhebung: Sammle Publikations- und Zitationslisten pro Person aus zwei Datenquellen (z. B. Scopus und OpenAlex) und dokumentiere Stichtag sowie Abdeckung. Autor:innen-Disambiguierung: Prüfe ORCID, Namensvarianten und Affiliationen, korrigiere Fehlzuordnungen und entferne Dubletten. Analyse: Berechne h-, g- und i10-Index sowie den m-Quotienten auf Basis des Karrierebeginns. Markiere Selbstzitate getrennt. Interpretation: Stelle die Kennzahlen in Kontext von Fachgebiet und Karrierestufe und liefere eine kurze Limitationen-Notiz. Wichtigste Erkenntnisse Durch saubere Autor:innenzuordnung und klare Regeln können Kennzahlen transparent kommuniziert werden, ohne sie als alleinige Leistungsindikatoren zu verwenden. Passende Module: Modul 4 (Autor:innenmetriken) Fallstudie 4: Science Mapping für strategische Themenfelder Eine Hochschule möchte neue Schwerpunktfelder identifizieren und visualisieren, welche Themencluster sich aus der Publikationslandschaft ergeben. Datenbasis: Exportiere Publikationen der letzten zehn Jahre mit Keywords, Referenzen und Abstracts aus einer Datenbank. Vorbereitung: Bereinige Keywords, entferne Dubletten und wähle eine konsistente Terminologie für das Mapping. Science Mapping: Erstelle Co-Word- und Co-Citation-Netzwerke, bilde Cluster und visualisiere sie mit passenden Tools. Interpretation: Identifiziere zentrale Cluster, Randthemen und aufstrebende Felder als Grundlage für Strategiegespräche. Wichtigste Erkenntnisse Die Clusteranalyse zeigt, welche Themen sich stark vernetzen und wo neue Forschungsschwerpunkte entstehen können. Interaktives Beispiel ähnlich zu Science-Mapping-Demonstrationen wie bei thinkik.github.io: Laden Sie Beispieldaten herunter oder importieren Sie eigene Netzwerkdaten direkt im Browser. Daten laden Beispieldaten laden Eigene Datei (JSON oder CSV) Beispiel-JSON herunterladen Beispiel-CSV herunterladen Cluster filtern Kantengewicht Minimum: 1 Filter zurücksetzen Lade Daten … Passende Module: Modul 11 (Science Mapping) Fallstudie 5: Dashboard-Reporting für eine Fakultät Ein Dekanat benötigt ein Quartals-Dashboard, das Publikationsoutput, Impact und Kooperationen in einer konsistenten Storyline zusammenführt. Kennzahlen-Set: Definiere Kernmetriken (Output, Zitationen, Open-Access-Anteil, internationale Kooperationen) und die Reporting-Logik. Visualisierung: Entwerfe ein Dashboard mit Trendlinien, Vergleichswerten und einer Seite „Interpretation & Limitationen“. Qualitätssicherung: Prüfe Plausibilität, dokumentiere Datenquellen und setze einen Stichtag für das Reporting. Nutzung: Bereite eine Executive Summary vor, die konkrete Handlungsempfehlungen aus den Zahlen ableitet. Wichtigste Erkenntnisse Das Dashboard schafft eine wiederholbare Reporting-Routine, die Zahlen und Kontext für Entscheidungen kombiniert. Passende Module: Modul 12 (Dashboard & Reporting) Die Fallstudien zeigen Zusammenhänge in Zitationsnetzwerken und Kollaborationen."
    },
    {
      "url": "tools-datenquellen.html",
      "title": "Tools & Datenquellen",
      "category": "Inhalte",
      "content": "Tools & Datenquellen Vergleiche wichtige bibliometrische Datenquellen. Du kannst nach Eigenschaften filtern und die Tabelle nach Kosten, Abdeckung, Transparenz oder Exportmöglichkeiten sortieren. Filter: Open Access kostenpflichtig hybrid Datenquelle Kosten Abdeckung Transparenz Exportmöglichkeiten Offizielle Website Web of Science Kostenpflichtig (Lizenz) Sehr hoch Hoch CSV, API (institutionell), InCites Website öffnen Scopus Kostenpflichtig (Lizenz) Sehr hoch Hoch CSV, RIS, API (lizenzabhängig) Website öffnen OpenAlex Open Access Hoch Sehr hoch REST-API, Dumps (JSONL), CSV via Tools Website öffnen Crossref Open Access Hoch Sehr hoch REST-API, JSON/XML-Metadaten Website öffnen Dimensions Hybrid (teils offen, teils Lizenz) Hoch Mittel CSV, API (lizenzabhängig), Dashboards Website öffnen Google Scholar Open Access (Suche) Hoch Niedrig Kein offizieller Bulk-Export/API Website öffnen Für die gewählten Filter wurden keine Datenquellen gefunden. Datenquellen, Schnittstellen und Tools greifen ineinander und bestimmen die Abdeckung."
    },
    {
      "url": "responsible-metrics.html",
      "title": "Responsible Metrics",
      "category": "Inhalte",
      "content": "Responsible Metrics Hier findest du die Inhalte für den Abschnitt „Responsible Metrics“ der Einführung Bibliometrie. Weitere Details folgen in diesem Bereich. DORA: Kernaussagen für faire Bewertung Bewerte Forschung nach Inhalt und Qualität, nicht nach Journalmetriken. Nutze eine Vielzahl von Evidenzen (Publikationen, Daten, Software, Wirkung). Transparenz über Datenquellen, Methoden und Grenzen von Kennzahlen. Berücksichtige Beiträge von Teams und unterschiedliche Rollen. Fallstrick: Journal-Impact-Faktoren als Ersatz für Qualität. So vermeiden: Qualitative Peer-Review-Kriterien ergänzen und Indikatoren nur als Kontext nutzen. Leiden-Manifest: Prinzipien für verantwortungsvolle Metriken Kontext beachten: Ziele, Fachkultur und Karrierephasen unterscheiden sich. Mehrere Indikatoren kombinieren, um Verzerrungen zu minimieren. Quantitative Daten mit Expertenwissen und Narrativen ergänzen. Transparenz und Offenheit bei Daten, Methoden und Auswertung. Regelmäßig prüfen, ob Metriken Nebenwirkungen erzeugen. Fallstrick: Ein einzelner Indikator bestimmt Entscheidungen. So vermeiden: Indikatoren-Set definieren und Begründungen dokumentieren. Praxis: So setzt du Responsible Metrics um Starte mit einer Zielklärung: Welche Entscheidungen sollen unterstützt werden? Erstelle ein Indikatoren-Set mit erklärten Grenzen (z. B. Publikationen, Open-Access-Anteile, Kollaboration). Dokumentiere Datenquellen, Datenstand und Bereinigungen nachvollziehbar. Kombiniere Kennzahlen mit Narrativen: Was wurde erreicht, was ist kontextrelevant? Plane Feedbackschleifen mit Fachcommunitys und Betroffenen ein. Fallstrick: Metriken ersetzen Diskussionen. So vermeiden: Kennzahlen nur als Gesprächsgrundlage nutzen und Entscheidungen begründen. Do No Harm: Fairness und Schutz sensibler Gruppen Vergleiche nie ohne Kontext: Karrierephase, Betreuungszeiten, Teilzeit und Fachkultur einbeziehen. Nutze Bandbreiten statt harte Schwellenwerte, um „Cliff Effects“ zu vermeiden. Erkenne Verzerrungen in Daten (z. B. Sprach-, Regional- oder Geschlechterbias) und dokumentiere sie. Beziehe Beiträge außerhalb traditioneller Publikationen ein (Open Science, Lehre, Community). Schütze sensible Informationen und nutze aggregierte Ergebnisse, wo möglich. Fallstrick: Early-Career-Forschende werden durch starre Vergleichsgruppen benachteiligt. So vermeiden: Vergleichsgruppen nach Karrierephase clustern und qualitative Evidenz ergänzen. Tools & Ressourcen COARA – Coalition for Advancing Research Assessment : Leitlinien und Commitment für verantwortliche Bewertung. HKRAM Guidelines : Rahmenempfehlungen für verantwortungsvolle Leistungsbewertung (nationaler Kontext). Leiden Manifesto und DORA : Grundprinzipien für die Bewertung von Forschung. Toolkit for Responsible Metrics : Praxisorientierte Checklisten, Beispiele und Methoden. Reflexionsfragen für Teams Welche Entscheidung soll unterstützt werden – und wer ist betroffen? Welche Verzerrungen entstehen durch unsere Datenquellen? Welche zusätzlichen Evidenzen sind nötig, um faire Entscheidungen zu treffen? Wie kommunizieren wir Unsicherheiten und Grenzen der Kennzahlen? Wie überprüfen wir regelmäßig unbeabsichtigte Nebenwirkungen? Dos & Don’ts für verantwortungsvolles Handeln Dos Mehrere Metriken nutzen und ihre Aussagekraft erklären. Kontext (Fach, Karrierephase, Ressourcen) konsequent berücksichtigen. Qualitative Einschätzungen mit Daten kombinieren. Transparente Kriterien und Feedbackschleifen etablieren. Don’ts Einzelmetriken als Ranking- oder Leistungsersatz verwenden. Indikatoren ohne Datenqualität und Fachnormen zu prüfen anwenden. Beiträge außerhalb klassischer Publikationen ignorieren. Metriken nutzen, ohne Betroffene einzubeziehen. Fallstrick: „Gaming“ durch Zielvorgaben für Kennzahlen. So vermeiden: Ziele mit qualitativen Kriterien koppeln und unerwünschte Effekte regelmäßig evaluieren. Responsible Metrics verbindet Daten, Kontext und Reflexion zu fairen Bewertungen."
    },
    {
      "url": "downloads-vorlagen.html",
      "title": "Downloads & Vorlagen",
      "category": "Inhalte",
      "content": "Downloads & Vorlagen Die folgenden Vorlagen unterstützen dich bei der strukturierten Datenerfassung, der Erstellung eines Methodenberichts und der konsistenten Dokumentation von Analyseergebnissen in der Bibliometrie. Datenaufbereitung.xlsx Tabellenstruktur zur Erfassung von Publikations-, Zitier- und Metadaten inklusive vorbereiteter Felder für Bereinigung und Normalisierung. Datei im Repo öffnen Herunterladen Bereit Methodenbericht.docx Dokumentvorlage für transparente Methodenbeschreibungen mit Abschnitten zu Datengrundlage, Auswahlkriterien, Limitationen und Reproduzierbarkeit. Datei im Repo öffnen Herunterladen Bereit Indikatoren Checkliste.pdf Checkliste zur Auswahl und Bewertung bibliometrischer Kennzahlen mit Kriterien für Aussagekraft, Vergleichbarkeit und verantwortungsvollen Einsatz. Datei im Repo öffnen Herunterladen Bereit Visualisierungsvorlage.pptx Foliensatz mit vordefinierten Diagramm-Layouts, Farbschema und Platzhaltern für die verständliche Präsentation bibliometrischer Ergebnisse. Datei im Repo öffnen Herunterladen Bereit Vorlagen, Checklisten und Leitfäden unterstützen die praktische Anwendung."
    },
    {
      "url": "index.html",
      "title": "Start",
      "category": "Inhalte",
      "content": "Start Dein Einstieg in die Bibliometrie Entdecke die wichtigsten Grundlagen, Datenquellen und Responsible-Metrics-Prinzipien auf einen Blick. Die Lernplattform führt dich Schritt für Schritt durch Anwendungen, Kennzahlen und Reflexionsfragen. Zum Lehrgang Responsible Metrics Einführung in die Lernplattform Die Lernplattform „Einführung Bibliometrie“ unterstützt dich dabei, bibliometrische Methoden sicher anzuwenden, Forschungsergebnisse kritisch einzuordnen und datenbasierte Entscheidungen nachvollziehbar zu begründen. Du erhältst einen praxisnahen Überblick über Kennzahlen, Datenquellen und Analyseworkflows, die in Bibliotheken, Forschungsservices und Wissenschaftsmanagement gefragt sind. Bibliometrische Kompetenzen helfen dabei, Publikationsstrategien zu bewerten, die Sichtbarkeit von Forschung zu steigern und verantwortungsvolle Metriken einzusetzen. Deshalb kombiniert der Kurs theoretische Grundlagen mit Übungen und Reflexionen zur verantwortungsvollen Nutzung von Indikatoren. Module im Überblick Modul 1: Grundlagen der Bibliometrie und zentrale Begriffe. Modul 2: Datenquellen, Tools und Auswertungsstrategien. Modul 3: Anwendung, Interpretation und Responsible Metrics. Auf der Startseite findest du die wichtigsten Einstiege und kannst direkt zu den Modulen wechseln. Modul 1 Modul 2 Modul 3"
    },
    {
      "url": "modul1.html",
      "title": "Modul 1: Grundlagen der Bibliometrie",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 1: Grundlagen der Bibliometrie Schwierigkeit: Beginner Geschätzte Lesezeit: 45 Minuten Zielgruppen: Bibliothek, Research Office, Forschende Voraussetzungen: Keine Wähle deine Rolle für passende Hinweise: Bibliothek Research Office Forschende Hinweis: Klärt mit dem Team, welche bibliometrischen Basisbegriffe in Schulungen konsistent verwendet werden sollen. Hinweis: Notiert typische Nutzerfragen, um spätere Beratungsangebote daran auszurichten. Hinweis: Haltet fest, welche Kennzahlen für interne Reports wirklich benötigt werden. Hinweis: Prüft früh, welche Datenquellen für zentrale Entscheidungen akzeptiert sind. Hinweis: Überlegt, welche Indikatoren eure Arbeit sinnvoll ergänzen, ohne sie zu reduzieren. Hinweis: Sammelt Beispiele, wo Zitationszahlen missverstanden wurden, um gegenzusteuern. Lernziele Grundbegriffe der Bibliometrie erklären Zitationsdaten als Indikatoren einordnen Einsatzbereiche und Grenzen benennen Kurz erklärt Bibliometrie untersucht wissenschaftliche Kommunikation anhand von Publikations- und Zitationsdaten. In diesem Modul lernen Sie, warum Zitationen als Hinweis auf Rezeption dienen, aber nicht mit Qualität gleichgesetzt werden dürfen. Wir betrachten typische Fragestellungen wie Forschungsleistung messen, Themenfelder beobachten oder Kooperationen sichtbar machen. Zugleich klären wir Begriffe wie Publikationstypen, Zitationsfenster, Datenbanken und die Rolle von Referenzen. Wir unterscheiden deskriptive Analysen von evaluativen Anwendungen und besprechen, welche Entscheidungen auf welcher Evidenz beruhen sollten. Ein besonderer Fokus liegt auf verantwortungsvollem Umgang mit Kennzahlen: Indikatoren sind Werkzeuge zur Orientierung, keine Urteile. Am Ende können Sie die Rolle der Bibliometrie im Forschungsmanagement einordnen und erkennen, welche Datenbasis für valide Aussagen nötig ist. Vertiefung Zentrale Begriffe und Definitionen sammeln und mit Beispielen illustrieren. Begrenzen, welche Aussagen mit den gewählten Daten tatsächlich möglich sind. Ein kurzes Reflexionsprotokoll zu Annahmen, Datenlücken und Bias erstellen. Praxis-Workflow Fragestellung präzisieren und Zielgruppe definieren. Datenquelle auswählen und dokumentieren. Indikatoren berechnen und validieren. Ergebnisse visualisieren und interpretieren. Bericht mit Limitationen veröffentlichen. Responsible-Metrics-Box Verwenden Sie mehrere Indikatoren und erläutern Sie deren Grenzen. Kombinieren Sie quantitative Werte mit qualitativen Einschätzungen. Typische Fehlinterpretationen Indikatoren als direkte Qualitätsurteile verwenden. Unterschiedliche Fächer oder Zeitfenster ohne Normalisierung vergleichen. Datenlücken oder Dubletten ignorieren. Übungen Level 1–3 Level 1 Ein zentrales Konzept in zwei Sätzen definieren. Level 2 Ein kleines Datenset skizzieren und passende Indikatoren vorschlagen. Level 3 Eine kurze Interpretation schreiben und mögliche Bias benennen. Quellen https://www.dora.org/ https://www.leidenmanifesto.org/ https://www.crossref.org/"
    },
    {
      "url": "modul2.html",
      "title": "Modul 2: Publikationsdaten und Quellen",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 2: Publikationsdaten und Quellen Schwierigkeit: Beginner Geschätzte Lesezeit: 50 Minuten Zielgruppen: Bibliothek, Research Office, Forschende Voraussetzungen: Modul 1 Wähle deine Rolle für passende Hinweise: Bibliothek Research Office Forschende Hinweis: Baut ein internes Standardblatt: „Welche Datenquelle für welche Frage?“ Hinweis: Dokumentiert Coverage-Hinweise sichtbar im Report (z. B. als Methodik-Kasten). Hinweis: Vermeidet Tool-Mix ohne Erklärung: Wenn mehrere Quellen kombiniert werden, müsst ihr Dubletten/IDs klären. Hinweis: Definiert früh den Zweck: Monitoring ≠ Bewertung. Die Datenquelle hängt am Ziel. Hinweis: Für Governance: verlangt Standardangaben (Quelle, Zeitraum, Dokumenttypen, Limitationen). Hinweis: Wenn Ranking-Druck entsteht: nutzt Responsible-Metrics-Checks (Kontext + Multi-Indikator + qualitative Evidenz). Hinweis: Wenn Zahlen nicht passen: prüfe zuerst Namensvarianten, Affiliations, Dokumenttypen und Zeitraum. Hinweis: Nutze Kennzahlen in CVs nur mit kurzer Kontextzeile (Quelle + Zeitraum + Feld). Hinweis: Für Themen-/Kooperationsanalysen können offene Quellen sehr nützlich sein – aber Ergebnisse immer plausibilisieren. Lernziele Die wichtigsten bibliometrischen Datenquellen unterscheiden. Verstehen, was „Coverage“ praktisch bedeutet. Erklären, warum Zahlen je nach Datenquelle unterschiedlich ausfallen können. Eine passende Datenquelle für eine konkrete Fragestellung auswählen und Limitationen kommunizieren. Kurz erklärt Bibliometrische Kennzahlen sind nur so gut wie ihre Datenbasis. Unterschiedliche Datenquellen erfassen unterschiedliche Publikationen, Sprachen und Dokumenttypen – und verknüpfen Zitationen nicht immer gleich. Deshalb können zwei seriöse Systeme für dieselbe Einheit verschiedene Publikations- und Zitierzahlen zeigen. In diesem Modul lernst du, welche Quellen es gibt, wie Coverage entsteht und wie du Ergebnisse transparent und verantwortungsvoll kommunizierst. Vertiefung Schlüsselbegriffe Datenquelle: Ein System/Index, aus dem bibliometrische Daten stammen (z. B. kuratierte Zitationsdatenbank oder offene Metadateninfrastruktur). Coverage (Abdeckung): Welche Inhalte eine Datenquelle enthält (Zeitschriften, Konferenzen, Bücher, Sprachen, Länder, Jahre, Dokumenttypen). Indexierung: Wie Inhalte aufgenommen, strukturiert und verknüpft werden (z. B. Zitationslinks, Autor:innen, Institutionen). Kuratiert: Inhalte werden nach definierten Kriterien ausgewählt und redaktionell betreut. Offene Infrastruktur: Offene Daten/Metadaten (z. B. via API), die reproduzierbare Analysen ermöglichen, aber stark von Datenqualität abhängen. Bias: Systematische Verzerrung in Daten (z. B. Sprache, Region, Disziplin, Dokumenttyp), die Ergebnisse beeinflussen kann. Die wichtigsten Datenquellen im Überblick In der Bibliometrie gibt es grob drei Familien von Datenquellen: 1) Kuratierte Zitationsdatenbanken (z. B. WoS, Scopus) Vorteil: definierte Selektions- und Indexierungslogik, konsistente Metadaten, stabile Auswertungen. Risiko: nicht jede Disziplin, Sprache oder Publikationsform ist gleich gut abgedeckt. 2) Offene Datenquellen (z. B. OpenAlex, Crossref) Vorteil: offen zugänglich, API-basiert, reproduzierbar. Risiko: Metadatenqualität ist heterogen; Coverage kann je nach Bereich schwanken. 3) Suchmaschinen/Plattformen (z. B. Google Scholar) Vorteil: häufig sehr breite Abdeckung. Risiko: begrenzte Transparenz, schwierig für belastbare Evaluation. Warum unterscheiden sich Zahlen je nach Quelle? Unterschiedliche Coverage (welche Journals/Konferenzen/Bücher enthalten sind) Unterschiedliche Dokumenttypen (z. B. Proceedings, Preprints) Unterschiedliche Indexierung (Zitationsverknüpfungen, Duplikate, Normalisierung) Aktuelle Entwicklungen (Open Science, Preprints, Datenzitation, 2024/25) Seit 2024/25 rücken Open-Science-Praktiken und Reforminitiativen der Forschungsbewertung stärker in den Fokus. Für Bibliometrie bedeutet das: Quellen müssen nicht nur Publikationen, sondern auch Preprints, Daten und weitere Outputs sauber abbilden – inklusive Versionierung, Peer-Review-Status und Nutzungsindikatoren. Preprints als eigene Dokumenttypen: Preprints (z. B. arXiv, bioRxiv, medRxiv) sind früh verfügbar und oft hoch sichtbar. Für Reporting gilt: Version (Preprint vs. Published), Peer-Review-Status und Doppelzählungen klar kennzeichnen. Datenzitation & Data-Citation-Indexes: Daten erhalten DOIs (DataCite) und werden über Indizes wie den Data Citation Index oder offene Infrastrukturen sichtbar. Wichtig sind konsistente Datenzitation und saubere Metadaten. Metriken jenseits von Zitationen: Usage (Views/Downloads, COUNTER-konform), Policy-Citations (z. B. Overton) und Open-Science-Signale (OA-Anteile, Datenverfügbarkeit) ergänzen klassische Zitationsmetriken. Bewertungspolitiken im Wandel: COARA, DORA und nationale Leitlinien verlangen mehr Kontext, Transparenz und qualitative Evidenz statt Einzelmetriken. Merksatz: Eine Kennzahl ohne Angabe der Datenquelle ist unvollständig. Mini-Checkliste für jeden Report Datenquelle(n): … Zeitraum: … Dokumenttypen: … Limitationen/Coverage-Hinweise: … Reproduzierbarkeit: Query/Datum dokumentiert: Ja/Nein Typische Fehlinterpretationen Mythos: „Wenn zwei Quellen unterschiedliche Zahlen liefern, ist eine falsch.\" Korrektur: Unterschiede entstehen oft durch Coverage, Dokumenttypen und Indexierungslogik. Mythos: „Mehr Coverage bedeutet automatisch bessere Bibliometrie.\" Korrektur: Transparenz, Datenqualität und definierte Regeln sind entscheidend. Mythos: „Wir können jede Disziplin gleich vergleichen.\" Korrektur: Coverage und Zitationskulturen unterscheiden sich stark. Mythos: „Tools liefern neutrale Wahrheit.\" Korrektur: Tools spiegeln Entscheidungen über Indexierung und Metadatenqualität wider. Praxis-Workflow: Datenquelle auswählen Frage klären: Monitoring, Reporting, Trendanalyse oder Evaluation? Analyseobjekt definieren: Person/Institut/Thema + Zeitraum + Publikationstypen. Kandidaten-Datenquellen auswählen (kuratierte DB vs. offen vs. Suchmaschine). Coverage prüfen: Disziplin, Sprache, Bücher/Proceedings, regionale Literatur. Testabfrage durchführen und plausibilisieren (Stichprobe gegen bekannte Publikationen). Bereinigung planen: IDs, Dubletten, Affiliations, Namensvarianten. Ergebnisbericht: immer mit Datenbasis + Limitationen + Datum der Abfrage. Responsible-Metrics-Box: Minimum-Transparenz für Datenquellen Nenne immer Datenquelle(n), Zeitraum und Dokumenttypen. Wenn Zahlen verglichen werden: erkläre Coverage-Unterschiede und warum der Vergleich sinnvoll ist (oder nicht). Vermeide mechanische Entscheidungen anhand einer einzigen Quelle oder Kennzahl. Nutze Bibliometrie als Hinweis-System: Ergebnisse sollten plausibilisiert und kontextualisiert werden. Übungen Level 1–3 Level 1 M02-L1-Q1: Was bedeutet „Coverage“ in einer bibliometrischen Datenquelle? Wie viele Personen an einem Institut arbeiten. Welche Publikationsarten, Disziplinen, Sprachen und Jahre in der Quelle enthalten sind. Wie hoch der Journal Impact Factor ist. Lösung: Welche Publikationsarten, Disziplinen, Sprachen und Jahre in der Quelle enthalten sind. M02-L1-Q2: Welche Aussage ist am besten im Sinne von Responsible Metrics? Ein Report braucht keine Methodik, Hauptsache die Zahl ist klar. Zu jeder Kennzahl gehören Datenquelle, Zeitraum und Limitationen. Wenn zwei Tools verschieden zählen, nimmt man einfach den höheren Wert. Lösung: Zu jeder Kennzahl gehören Datenquelle, Zeitraum und Limitationen. M02-L1-Q3: Wähle die defensibelste Erklärung: Warum zeigt Quelle A mehr Publikationen als Quelle B? Quelle A hat wahrscheinlich mehr Dokumenttypen oder breitere Abdeckung indexiert. Quelle B ist automatisch falsch. Das Institut hat heimlich Publikationen gelöscht. Lösung: Quelle A hat wahrscheinlich mehr Dokumenttypen oder breitere Abdeckung indexiert. M02-L1-Q4: Was ist ein sinnvoller erster Schritt, bevor du zwei Datenquellen vergleichst? Sofort ein Ranking erstellen. Dokumenttypen und Zeitraum angleichen und Coverage prüfen. Nur nach Gefühl entscheiden, welche Quelle besser ist. Lösung: Dokumenttypen und Zeitraum angleichen und Coverage prüfen. M02-L1-Q5: Warum ist die Datenquelle in jedem Bibliometrie-Report Pflichtangabe? Weil Zahlen nur im Kontext der Coverage interpretierbar sind. Weil sonst das Layout zu leer ist. Weil Kennzahlen überall identisch sind. Lösung: Weil Zahlen nur im Kontext der Coverage interpretierbar sind. Level 2 M02-L2-Q1: Interpretation: Zwei Quellen liefern unterschiedliche Publikationszahlen. Was schreibst du in den Report (2–3 Sätze, defensiv formuliert)? Einheit Zeitraum Quelle Publikationen Institut A 2021–2024 Quelle 1 120 Institut A 2021–2024 Quelle 2 155 Lösung: Die unterschiedlichen Werte sind wahrscheinlich durch abweichende Coverage erklärbar. Für die Interpretation ist entscheidend, welche Datenquelle zur Fragestellung passt. Im Report werden Datenquelle, Zeitraum, Dokumenttypen und Limitationen transparent dokumentiert. M02-L2-Q2: Quelle auswählen: Du brauchst ein reproduzierbares Monitoring-Dashboard ohne Paywall. Welche Datenquelle passt am ehesten und welche Einschränkung nennst du? Lösung: OpenAlex (API-basiert, offen, reproduzierbar) ist naheliegend. Einschränkung: Coverage und Metadatenqualität können variieren; deshalb braucht es Plausibilisierung und Dokumentation der Abfrageparameter. M02-L2-Q3: Quality Check: Nenne 3 Prüfungen, die du bei einer Publikationsliste aus einer Datenquelle machst, bevor du Kennzahlen berichtest. Lösung: Beispiele: Dublettenprüfung, Namensvarianten/IDs plausibilisieren, Affiliations prüfen, Dokumenttypen filtern, Zeitraum korrekt setzen. Level 3 M02-L3-CASE: Mini-Case: Du sollst für eine Fakultät einen Vergleich „Publikationsoutput 2020–2024\" erstellen. Definiere deine Datenbasis (Quelle(n), Dokumenttypen, Zeitraum) und formuliere 4 Limitationen/Warnhinweise. Deliverable: Methodik-Kasten + 4 Limitationen (Bulletpoints). Quellen COARA (2022): Agreement on Reforming Research Assessment: https://www.coara.eu/agreement/ Clarivate: Web of Science Core Collection – Content collection and indexing process: https://clarivate.com/academia-government/scientific-and-academic-research/research-discovery-and-referencing/web-of-science/web-of-science-core-collection/content-collection-and-indexing-process/ Elsevier: Scopus Content Coverage Guide (PDF): https://assets.ctfassets.net/o78em1y1w4i4/EX1iy8VxBeQKf8aN2XzOp/c36f79db25484cb38a5972ad9a5472ec/Scopus_ContentCoverage_Guide_WEB.pdf OpenAlex Documentation: API Overview: https://docs.openalex.org/how-to-use-the-api/api-overview Crossref Documentation: Metadata Retrieval: https://www.crossref.org/documentation/retrieve-metadata/ DataCite: Data Citation and DOIs for research data: https://datacite.org/ Clarivate: Data Citation Index: https://clarivate.com/webofsciencegroup/solutions/data-citation-index/ arXiv (Preprints): https://arxiv.org/ bioRxiv (Preprints): https://www.biorxiv.org/ medRxiv (Preprints): https://www.medrxiv.org/ Project COUNTER: Code of Practice (Usage-Standard): https://cop5.projectcounter.org/ Overton (Policy citations): https://www.overton.io/ CWTS (Leiden): Bibliometrics for Research Management (PDF): https://www.cwts.nl/pdf/CWTS_bibliometrics.pdf Gusenbauer & Haddaway (2020): Suitable academic search systems (PubMed): https://pubmed.ncbi.nlm.nih.gov/31614060/ Gusenbauer (2024): Beyond Google Scholar, Scopus, and WoS: https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1729 Weiterführende Lektüre López-Cózar (2018): Google Scholar as bibliographic tool (arXiv PDF): https://arxiv.org/pdf/1806.06351 Dimensions Support: What is covered in Publications?: https://plus.dimensions.ai/support/solutions/articles/23000018859-what-exactly-is-covered-in-the-publications-in-dimensions-"
    },
    {
      "url": "modul3.html",
      "title": "Modul 3: Zitationsindikatoren",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 3: Zitationsindikatoren Schwierigkeit: Beginner Geschätzte Lesezeit: 55 Minuten Zielgruppen: Bibliothek, Forschungsreferat, Forschende Voraussetzungen: Modul 1, Modul 2 Wähle deine Rolle für passende Hinweise: Bibliothek Research Office Forschende Hinweis: In Beratungen zuerst Datenbasis prüfen, dann interpretieren. Hinweis: Zitationen immer mit Zitationsfenster und Abfragedatum ausweisen. Hinweis: Wenn möglich, Standardformulierung für Zitationszahlen nutzen. Hinweis: Zitationszahlen als Trend- und Kontextsignal nutzen, nicht als Ranking. Hinweis: Bestehe auf Mindest-Transparenz (Quelle, Fenster, Selbstzitate, Dokumenttypen). Hinweis: Bei Entscheidungen auf Multi-Methoden-Ansatz setzen (quantitativ + qualitativ). Hinweis: Vergleiche nur innerhalb sinnvoller Gruppen (Fach, Karrierestufe, Publikationstyp, Alter). Hinweis: Kontextzeile mit Quelle + Zeitraum + Fenster ergänzen. Hinweis: Bei Peaks prüfen, ob Reviews/Methodenpapiere oder Datenartefakte vorliegen. Lernziele Zitationen als Signal (nicht als Beweis) verstehen. Zitationsfenster und Zeitdynamiken erklären. Selbstzitate und Zitationskontext korrekt einordnen. Zitationskennzahlen transparent berichten. Kurz erklärt Eine Zitation ist ein Verweis von einer Publikation auf eine andere. Zitationszahlen werden oft genutzt, um Resonanz oder Sichtbarkeit zu beschreiben. Aber: Zitationen sind kein direkter Qualitätsbeweis. Sie hängen vom Fachgebiet, vom Alter eines Artikels, vom Publikationstyp und von der Datenquelle ab. Zusätzlich sind nicht alle Zitationen „Lob“ – manche sind neutral oder kritisch. Deshalb brauchst du beim Interpretieren immer Kontext: Zitationsfenster, Fachgebiet, Datenbasis und klare Limitationen. Vertiefung Schlüsselbegriffe Zitation (Citation): Verweis auf eine Publikation; Signal für Resonanz, nicht automatisch Qualität. Zitationszählung: Anzahl Zitationen in einer Datenquelle innerhalb eines Zeitraums. Zitationsfenster: Zeitraum, in dem Zitationen gezählt werden (z. B. 2, 4, 5 oder 10 Jahre). Zitationsverzögerung: Zeit, bis eine Publikation zitiert wird. Selbstzitation: Zitationen, bei denen sich Autor:innen selbst zitieren. Zitationskontext: Verwendung der Zitation (zustimmend, neutral, kritisch). Feldunterschiede: Unterschiede in Publikations- und Zitationskulturen. Wie Zitationsdaten entstehen – und warum sie tückisch sein können Zeitdynamik (Alter zählt): Neuere Publikationen hatten weniger Zeit, Zitationen zu sammeln. Darum ist das Zitationsfenster ein zentraler Parameter. Verteilung: Zitationen sind stark ungleich verteilt; Durchschnittswerte können irreführend sein. Dokumenttyp-Effekt: Reviews und Methodenpapiere werden oft häufiger zitiert als Spezialstudien. Selbstzitate: Legitimes Anschlusszitat, kann Kennzahlen verzerren. Gute Praxis: mit und ohne Selbstzitate ausweisen. Zitationskontext: Zitationen können zustimmend, neutral oder kritisch sein; nicht jede Zitation ist „positiv“. Praktische Konsequenz: Zitationszahlen immer mit Datenquelle, Abfragedatum, Zeitraum/Zitationsfenster, Dokumenttypen, Selbstzitate-Regel und Limitationen dokumentieren. Praxis-Workflow: Zitationen korrekt berichten Ziel klären: Monitoring, Kontext im CV, Report oder Evaluation? Datenquelle festlegen (und begründen) + Abfragedatum dokumentieren. Zitationsfenster definieren und konsistent halten. Dokumenttypen festlegen (z. B. Article/Review getrennt ausweisen). Selbstzitate-Regel definieren (mit/ohne) und transparent berichten. Ergebnisse plausibilisieren (Stichprobe, Ausreißer prüfen, Kontext beachten). Methodik-Kasten + Limitationen + Responsible-Metrics-Hinweisbox einfügen. Responsible Metrics: Zitationen Zitationen sind Kontextsignale, keine automatische Qualitätsnote. Zitationsfenster und Alter sind Pflichtkontext für jede Interpretation. Dokumenttypen und Fachgebiet berücksichtigen. Selbstzitate transparent behandeln. Bei Evaluationen: Zitationen nur zusammen mit qualitativen Verfahren nutzen. Typische Fehlinterpretationen „Viele Zitationen beweisen hohe Qualität.\" Zitationen sind ein Signal, beeinflusst durch Feld, Alter, Dokumenttyp und Kontext. „Neuere Publikationen sind schwächer.\" Oft ist das ein Zeitfenster-Effekt; Vergleiche brauchen passende Fenster. „Selbstzitate sind immer Betrug.\" Häufig legitim, wichtig ist Transparenz. „Alle Zitationen sind positiv.\" Zitationen können neutral oder kritisch sein. „Man kann jedes Fachgebiet direkt vergleichen.\" Zitationskulturen unterscheiden sich stark. Übungen Level 1–3 Level 1 M03-L1-Q1: Welche Aussage zu Zitationen ist am defensibelsten? Viele Zitationen beweisen, dass die Forschung qualitativ besser ist. Zitationen können Sichtbarkeit anzeigen, müssen aber mit Kontext interpretiert werden. Zitationen sind wertlos und sollten nie verwendet werden. Lösung: Zitationen können Sichtbarkeit anzeigen, müssen aber mit Kontext interpretiert werden. M03-L1-Q2: Was ist ein Zitationsfenster (Citation Window)? Die Anzahl Referenzen im Literaturverzeichnis. Der Zeitraum, in dem Zitationen gezählt werden (z. B. 4 Jahre). Die Anzahl Publikationen pro Jahr. Lösung: Der Zeitraum, in dem Zitationen gezählt werden (z. B. 4 Jahre). M03-L1-Q3: Welche Aussage zu Selbstzitaten passt am besten? Selbstzitate sind immer Betrug. Selbstzitate können legitim sein, sollten aber je nach Zweck transparent behandelt werden. Selbstzitate zählen nie in Datenquellen. Lösung: Selbstzitate können legitim sein, sollten aber je nach Zweck transparent behandelt werden. M03-L1-Q4: Welche Aussage ist korrekt? Alle Zitationen sind positive Anerkennung. Zitationen können auch kritisch oder neutral sein. Negative Zitationen sind unmöglich. Lösung: Zitationen können auch kritisch oder neutral sein. M03-L1-Q5: Warum sind Vergleiche zwischen Fachgebieten schwierig? Weil Zitations- und Publikationskulturen je nach Fach unterschiedlich sind. Weil alle Fächer identisch publizieren. Weil Datenquellen immer vollständig sind. Lösung: Weil Zitations- und Publikationskulturen je nach Fach unterschiedlich sind. Level 2 M03-L2-Q1: Zitationsfenster-Effekt: Welche Interpretation ist korrekt? Paper Year Citations in 2024 (cum.) A 2019 45 B 2023 8 Lösung: Paper A hatte mehr Zeit, Zitationen zu sammeln. Ein direkter Vergleich ohne gleiches Zitationsfenster oder Altersklassen ist unfair. Für einen faireren Vergleich sollte man Zitationen innerhalb eines definierten Fensters betrachten. M03-L2-Q2: Selbstzitate: Formuliere eine Report-Zeile (1–2 Sätze), die Zitationen mit und ohne Selbstzitate transparent ausweist. Einheit Zeitraum Zitationen (gesamt) Zitationen (ohne Selbstzitate) Institut A 2020–2024 980 840 Lösung: Im Zeitraum 2020–2024 wurden 980 Zitationen gezählt; ohne Selbstzitate sind es 840. Die Differenz wird zur Transparenz separat ausgewiesen. M03-L2-Q3: Dokumenttyp-Effekt: Warum kann ein Review-Artikel Zitationszahlen verzerren? Nenne 2 Gründe. Lösung: Reviews bündeln Literatur und werden oft als Einstieg zitiert. Außerdem werden Methoden- und Übersichtsarbeiten quer über Teilfelder zitiert, was Vergleiche verzerren kann. Level 3 M03-L3-CASE: Mini-Case: Formuliere einen Methodik-Kasten (Quelle, Abfragedatum, Zitationsfenster, Dokumenttypen, Umgang mit Selbstzitaten) und 4 Limitationen/Warnhinweise zur Interpretation. Quellen Garfield, E. (1955). Citation Indexes for Science. Science (PDF). https://garfield.library.upenn.edu/papers/science1955.pdf Hicks, D. et al. (2015). The Leiden Manifesto for research metrics. Nature. https://www.nature.com/articles/520429a DORA (2012). San Francisco Declaration on Research Assessment. https://sfdora.org/read/ Wilsdon, J. et al. (2015). The Metric Tide (PDF). https://www.ukri.org/wp-content/uploads/2021/12/RE-151221-TheMetricTideFullReport2015.pdf Waltman, L. (2016). A review of the literature on citation impact indicators. Journal of Informetrics. https://www.sciencedirect.com/science/article/pii/S1751157715300900 Fowler, J. H., & Aksnes, D. W. (2007). Does self-citation pay? Scientometrics. https://link.springer.com/article/10.1007/s11192-007-1777-2 Moed, H. F. (2007). Developing Bibliometric Indicators of Research Performance (CWTS report, PDF). https://www.cwts.nl/pdf/nwo_inf_final_report_v_210207.pdf Xu, J. et al. (2015). Citation Sentiment Analysis in Clinical Trial Papers (PMC). https://pmc.ncbi.nlm.nih.gov/articles/PMC4765697/ Song, D. et al. (2022). Quantifying characteristics of negative citations. Information Processing & Management. https://www.sciencedirect.com/science/article/abs/pii/S0306457322001108 Weiterführende Lektüre Waltman (Preprint). A review of the literature on citation impact indicators (arXiv). https://arxiv.org/abs/1507.02099"
    },
    {
      "url": "modul4.html",
      "title": "🚀 Modul 4: Metriken für Autor:innen",
      "category": "Module",
      "content": "Fortschritt: 0% 0% 🚀 Modul 4: Metriken für Autor:innen 🚀 Modul 4: Metriken für Autor:innen 📌 Auf einen Blick Eigenschaft Details Schwierigkeit 🟡 Fortgeschritten (Intermediate) Lesezeit ⏱️ ca. 25 Minuten Zielgruppe 🏛️ Bibliothek, Forschungsreferat, Forschende Voraussetzungen 📚 Module 1, 2 und 3 --- 🎯 Lernziele Nach Abschluss dieses Moduls können Sie: Die Logik und die Grenzen von h-Index, g-Index, i10-Index und m-Quotient verständlich erklären (und wissen, was sie *nicht* messen). Einfache Metriken aus einer Zitationsliste korrekt berechnen . Typische Verzerrungen erkennen (z. B. durch Karrierelänge, Dokumenttyp, Datenquellen, Namensvarianten oder Selbstzitate). Metriken report-tauglich kommunizieren (inklusive Quelle, Stichtag, Zeitraum und Limitationen). --- 💡 Kurz erklärt: Autor:innenmetriken Autor:innenmetriken sollen ein kompaktes Signal für Publikations- und Zitationsmuster liefern. Sie sind hilfreich, um sich einen Überblick zu verschaffen und Entwicklungen zu monitoren – jedoch brandgefährlich als alleinige Bewertungsgrundlage . Die Werte hängen massiv von der jeweiligen Datenquelle, der Karrierelänge, dem Dokumenttyp (z. B. Reviews werden generell häufiger zitiert) und einer sauberen Autor:innen-Zuordnung ab. > Best Practice: Verwenden Sie niemals nur eine einzelne Kennzahl. Liefern Sie stets den Kontext, die methodischen Spielregeln und die Limitationen mit! 🔑 Wichtige Schlüsselbegriffe vorab Autor:innen-Disambiguierung: Die saubere Zuordnung von Publikationen zu einer bestimmten Person (Bereinigung von Namensvarianten, unterschiedlichen Affiliations und Doppelprofilen). Datenquelle: Der Ursprung der Zitations- und Publikationsdaten (z. B. Web of Science, Scopus, Google Scholar, OpenAlex). --- 📊 Die 4 wichtigsten Metriken im Detail 1️⃣ h-Index Was er abbildet: Breite, anhaltende Resonanz (belohnt kontinuierliche Leistung, nicht nur ein einzelnes „One-Hit-Wonder“). Berechnung: Sortieren Sie alle Publikationen absteigend nach Zitationen. Der h-Index ist die größte Position *h*, an der gilt: Die Publikation hat mindestens *h* Zitationen. ✅ Stärken: Robust gegenüber einzelnen extremen Ausreißern; intuitiv und sehr einfach zu erklären. ⚠️ Grenzen: Bevorzugt längere Karrieren; unfair beim Vergleich unterschiedlicher Fachdisziplinen; ignoriert den Kontext der Zitationen; extrem abhängig von der Datenabdeckung der Quelle. 2️⃣ g-Index Was er abbildet: Im Gegensatz zum h-Index werden hier sehr stark zitierte Arbeiten (Top-Publikationen) stärker ins Gewicht geworfen. Berechnung: Sortieren Sie absteigend und bilden Sie die kumulierte Summe der Zitationen. Der g-Index ist die größte Zahl *g*, bei der die Top-*g* Publikationen zusammen mindestens *g²* Zitationen aufweisen. ✅ Stärken: Reagiert sensibel auf „Big Hits“; differenziert deutlich besser zwischen Profilen, die denselben h-Index aufweisen. ⚠️ Grenzen: Noch stärker von der Datenquelle abhängig als der h-Index; kann durch wenige extrem oft zitierte Arbeiten dominiert werden. 3️⃣ i10-Index Was er abbildet: Eine sehr einfache Schwelle für Produktivität und grundlegende Resonanz (wird vor allem in Google Scholar genutzt). Berechnung: Anzahl der Publikationen einer Person, die jeweils mindestens 10-mal zitiert wurden. ✅ Stärken: Extrem leicht zu erklären; bietet einen sehr schnellen Überblick. ⚠️ Grenzen: Sehr grobes Maß; stark feld- und datenquellenabhängig; außerhalb des Google-Universums wenig etabliert. 4️⃣ m-Quotient (m-Index) Was er abbildet: Eine grobe Normalisierung der Karrierezeit (h-Index pro Jahr). Berechnung: *m = h-Index ÷ (Jahre seit der allerersten Publikation)*. ✅ Stärken: Hilft als grobe Näherung beim Vergleich von Forschenden mit unterschiedlicher Seniorität (Karrierealter). ⚠️ Grenzen: Zu vereinfachend, da Karrieren und Zitationszyklen selten linear verlaufen; Feld- und Dokumenttyp-Effekte bleiben bestehen; die Definition des \"Startjahres\" muss klar definiert sein. --- 🛑 Typische Fallen & Fehlinterpretationen ❌ „Eine Zahl reicht, um Leistung zu bewerten.“ *Realität:* Metriken sind reine Kontextsignale. „Responsible Metrics“ verlangen immer mehrere Indikatoren kombiniert mit qualitativer Evidenz. ❌ „Der h-Index aus Quelle A ist identisch mit dem aus Quelle B.“ *Realität:* Datenbanken unterscheiden sich in ihrer Abdeckung (Coverage) und Zitationsverknüpfung erheblich. Werte aus verschiedenen Quellen sind niemals direkt vergleichbar. ❌ „Ein niedriger h-Index bedeutet schlechte Forschung.“ *Realität:* Karrierelänge, Fachgebiet, Dokumenttyp, Publikationssprache und die gewählte Datenbasis beeinflussen den Wert massiv. ❌ „Autor:innenprofile in Datenbanken stimmen automatisch.“ *Realität:* Namensvarianten und Doppelprofile sind der absolute Standard. Ohne manuelle Bereinigung ist jede darauf basierende Kennzahl völlig wacklig. --- 🛠️ Praxis-Workflow: Metriken sauber erheben Um Metriken professionell und belastbar zu nutzen, folgen Sie diesem 7-Schritte-Plan: Zweck klären: Dienen die Daten dem reinen Monitoring, als Kontext für einen CV, für einen offiziellen Bericht oder für eine Evaluation? Datenquelle festlegen: Quelle(n) eindeutig bestimmen und dokumentieren (inklusive exaktem Stichtag der Abfrage!). Profil bereinigen: Namensvarianten, Dubletten und falsche Zuordnungen korrigieren (Autor:innen-Disambiguierung). Regeln definieren: Welcher Zeitraum / welches Zitationsfenster gilt? Welche Dokumenttypen werden mitgezählt? Wie wird mit Selbstzitaten umgegangen? Werte berechnen: Metriken (h, g, i10, ggf. m) ermitteln. Plausibilisieren: Ausreißer kritisch prüfen (z. B. stark zitierte Guidelines, stark umstrittene Paper) und Stichproben gegen die tatsächliche Publikationsliste abgleichen. Report erstellen: Ergebnisse stets zusammen mit einem Methodik-Kasten , den Limitationen und einer verantwortungsvollen Interpretation präsentieren. --- ⚖️ Responsible Metrics: Der Mindeststandard Kontext ist König: Nutzen Sie niemals eine Kennzahl isoliert. Setzen Sie immer auf mehrere Signale. Maximale Transparenz: Geben Sie immer die genutzte Datenquelle, den Stichtag, den Zeitraum/das Fenster, die einbezogenen Dokumenttypen und die Berechnungsregeln an. Sinnvolle Vergleiche: Vergleichen Sie nur innerhalb plausibler Kohorten (gleiches Fachgebiet, identische Karrierestufe, gleiche Dokumenttypen). Qualität vor Quantität: Wenn weitreichende Entscheidungen anstehen (z. B. Berufungen, Fördermittel), dürfen quantitative Indikatoren nur unterstützend zur qualitativen Begutachtung (Peer Review) eingesetzt werden. --- 📝 Übungen (Level 1) Testen Sie Ihr erworbenes Wissen mit diesen kurzen Kontrollfragen: Frage 1: Welche Definition trifft den h-Index am besten? [ ] Anzahl der Publikationen insgesamt. [x] *h* Publikationen haben jeweils mindestens *h* Zitationen. [ ] Durchschnittliche Zitationen pro Publikation. Frage 2: Was misst der i10-Index? [ ] Durchschnittliche Zitationen pro Jahr. [x] Anzahl der Publikationen mit mindestens 10 Zitationen. [ ] Anzahl der Zitationen im letzten Jahr. Frage 3: Welche Aussage entspricht den Grundsätzen von \"Responsible Metrics\"? [ ] Wir ranken Forschende ausschließlich nach ihrem h-Index. [x] Wir nennen Quelle, Stichtag sowie Regeln und interpretieren den Wert im Kontext. [ ] Wenn zwei Datenbanken abweichen, nehmen wir einfach immer den höheren Wert."
    },
    {
      "url": "modul5.html",
      "title": "Modul 5: Journalmetriken: JIF, CiteScore, SNIP, SJR – korrekt nutzen, sauber kommunizieren",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 5: Journalmetriken: JIF, CiteScore, SNIP, SJR – korrekt nutzen, sauber kommunizieren Modul 5: Journalmetriken: JIF, CiteScore, SNIP, SJR – korrekt nutzen, sauber kommunizieren Schwierigkeit: Intermediate Geschätzte Lesezeit: 30 Minuten Zielgruppen: Bibliothek, Forschungsreferat, Forschende Voraussetzungen: Modul 1, Modul 2, Modul 3, Modul 4 Lernziele Die Logik und Unterschiede von Journal Impact Factor (JIF), CiteScore, SNIP und SJR erklären. Verstehen, warum Journalmetriken nicht zur Bewertung einzelner Artikel oder Personen taugen (Responsible Metrics). Typische Verzerrungen erkennen (Feldunterschiede, Dokumenttypen, Zitationsfenster, Datenquelle, Editorial Content). Journalmetriken report-tauglich und transparent kommunizieren (Quelle, Jahr, Fenster, Zählregeln, Limitationen). Schlüsselbegriffe Journal Impact Factor (JIF): Zitations-basierte Journalmetrik aus Journal Citation Reports (Clarivate). Vereinfacht: Zitationen in einem Jahr auf „recent citable items“ der Vorjahre geteilt durch Anzahl dieser „citable items“. Citable items: Dokumenttypen, die in den JIF-Denominator eingehen (z. B. meist Artikel/Reviews; genaue Regeln sind datenquellen-/anbieterabhängig). CiteScore: Journalmetrik auf Basis von Scopus: durchschnittliche Zitationen pro Dokument über ein vierjähriges Zeitfenster. SNIP: Source Normalized Impact per Paper/Publication: normalisiert Zitationen, um Unterschiede in Zitationspraktiken zwischen Feldern zu berücksichtigen. SJR: SCImago Journal Rank: gewichtet Zitationen nach „Prestige“ der zitierenden Journals (Netzwerklogik). Subject categories / Feldunterschiede: Journalmetriken sind stark feldabhängig; Vergleiche sollten innerhalb sinnvoller Fachgruppen erfolgen. Kurz erklärt Journalmetriken (JIF, CiteScore, SNIP, SJR) sind Kennzahlen auf Zeitschriftenebene. Sie können helfen, Journals innerhalb eines Fachgebiets zu vergleichen, Portfolios zu monitoren oder für Bibliotheksentscheidungen zusätzliche Evidenz zu liefern. Sie sind aber ungeeignet, einzelne Artikel oder Personen zu bewerten. Gute Praxis: immer mehrere Signale nutzen (Metriken + Scope/Fit + Peer Review/Qualitätssignale) und transparent dokumentieren, wie die Kennzahl zustande kommt. Vertiefung: Vier Metriken – vier Logiken 1) Journal Impact Factor (JIF) Idee: Wie häufig werden „durchschnittliche“ Artikel eines Journals zitiert? Wichtig: Der JIF hängt an Definitionen (welche Dokumenttypen zählen als „citable items“?) und an der Datenbasis (Web of Science/JCR). 2) CiteScore (Scopus) Idee: Durchschnittliche Zitationen pro Dokument über ein längeres (typisch vierjähriges) Zeitfenster. Praktisch: Vergleich innerhalb eines Felds; breitere Abdeckung von seriellen Titeln. 3) SNIP (Source Normalized Impact …) Idee: Normalisierung, um feldspezifische Zitationspraktiken zu berücksichtigen, ohne eine harte Feldklassifikation vorauszusetzen. Nutzen: Hilfreich, wenn Journals aus unterschiedlich „zitierfreudigen“ Bereichen verglichen werden müssen (mit Vorsicht). 4) SJR (SCImago) Idee: Nicht jede Zitation zählt gleich – Zitationen aus „prestigeträchtigen“ Journals werden stärker gewichtet (Netzwerk-/Prestige-Transfer). Nutzen: Ergänzende Perspektive (Prestige statt reinem Zitationsdurchschnitt). Merksatz: > Journalmetriken sind Journal-Signale – keine Artikel- oder Personen-Scores. Metriken kompakt JIF (JCR) Was er abbildet: Kurzfristige durchschnittliche Zitationshäufigkeit (journal-level). Fenster: typisch 2 Jahre (klassisch, je nach JCR-Definition). Stärken: Sehr verbreitet Gut dokumentiert im JCR-Kontext Grenzen: Feldabhängig Sensibel für Dokumenttypen/„citable items“ Nicht geeignet für Artikel-/Personenbewertung CiteScore (Scopus) Was er abbildet: Durchschnittliche Zitationen pro Dokument über ein breiteres Zeitfenster. Fenster: typisch 4 Jahre. Stärken: Längeres Fenster (stabiler) Breite Abdeckung serieller Titel Grenzen: Feldabhängig Datenquellenabhängig (Scopus) Definitionen/Updates über die Zeit beachten SNIP Was er abbildet: Zitationsimpact mit feldsensitiver Normalisierung. Fenster: i. d. R. mehrjährig (methodisch definiert). Stärken: Berücksichtigt Unterschiede in Zitationspraktiken Nützlich als Zusatzsignal Grenzen: Methodisch komplex Nicht als alleinige Entscheidungsbasis Abhängigkeit von sauberer Datenbasis SJR Was er abbildet: Prestigegewichteter Impact (Zitationsnetzwerk). Fenster: mehrjährig (methodisch definiert). Stärken: Gewichtet Herkunft der Zitationen Ergänzt reine Durchschnittsmetriken Grenzen: Schwer intuitiv Feld- und datenquellenabhängig Nicht als Artikel-/Personenscore missbrauchen Typische Fehlinterpretationen „JIF/CiteScore beweist die Qualität eines einzelnen Artikels.“ Journalmetriken sind Aggregatwerte. Artikel streuen stark; die Kennzahl sagt nichts Sicheres über den einzelnen Artikel aus. „Man kann Journals feldübergreifend anhand einer Zahl ranken.“ Zitationskulturen unterscheiden sich. Vergleiche gehören in Fachgruppen und mit Kontext. „Ein Journal mit hoher Metrik passt immer als Publikationsort.“ Scope/Fit, Zielpublikum, Review-Kultur, Open-Access-Optionen, Ethik/Transparenz sind genauso wichtig. „Wenn zwei Metriken widersprechen, ist eine falsch.“ Metriken messen Unterschiedliches (Fenster, Gewichtung, Normalisierung). Unterschiede sind erwartbar. Praxis-Workflow: Journalmetriken sinnvoll einsetzen Zweck klären: Journal-Auswahl, Portfolio-Management, Bibliotheksentscheidung, Reporting (nicht: Personenranking). Feld/Subject Category festlegen (Vergleichsgruppe definieren). Metrik-Set wählen (z. B. JIF + CiteScore + SNIP/SJR als Ergänzung). Qualitative Kriterien ergänzen: Scope/Fit, Peer-Review-Prozess, Editorial Policies, OA-Modelle, Ethik/Transparenz. Ergebnisse transparent berichten: Quelle, Jahr, Fenster, Zählregeln, Limitationen. Responsible-Metrics-Check: Keine mechanischen Entscheidungen; wenn high-stakes, qualitative Begutachtung einbeziehen. Responsible Metrics: Journalmetriken Keine Journalmetrik als Ersatz für Artikel- oder Personenbewertung verwenden. Immer Kontext: Fachgruppe, Datenquelle, Jahr und Zählregeln dokumentieren. Mehrere Indikatoren + qualitative Kriterien kombinieren. Mechanische Zielsteuerung (z. B. „publiziere nur in JIF>…“) vermeiden. Übungen Level 1 M05-L1-Q1: Welche Aussage ist korrekt? Journalmetriken bewerten einzelne Artikel zuverlässig. Journalmetriken sind Aggregatwerte auf Zeitschriftenebene. SNIP ist identisch mit JIF. Lösung: Journalmetriken sind Aggregatwerte auf Zeitschriftenebene. Bewertung: 1 Punkt: richtige Antwort. M05-L1-Q2: Welche Metrik nutzt typischerweise ein vierjähriges Zeitfenster? JIF CiteScore SJR (immer 2 Jahre) Lösung: CiteScore Bewertung: 1 Punkt: richtige Antwort. M05-L1-Q3: Welche Aussage passt zu SNIP am besten? SNIP ignoriert Feldunterschiede komplett. SNIP versucht Unterschiede in Zitationspraktiken zwischen Feldern zu berücksichtigen. SNIP ist eine reine Prestigegewichtung wie PageRank. Lösung: SNIP versucht Unterschiede in Zitationspraktiken zwischen Feldern zu berücksichtigen. Bewertung: 1 Punkt: richtige Antwort. Level 2 M05-L2-CALC-1: Rechenübung (vereinfachtes Beispiel): JIF für Jahr 2025. Gegeben: 480 Zitationen im Jahr 2025 auf citable items aus 2023–2024. Citable items 2023–2024: 160. Wie groß ist der JIF (2025)? Lösung: JIF = 480 / 160 = 3.0 Bewertung: 1 Punkt: korrekter Wert. 1 Punkt: korrekte Rechnung. M05-L2-CALC-2: Rechenübung (vereinfachtes Beispiel): CiteScore (4-Jahresfenster). Gegeben: 1'200 Zitationen im 4-Jahresfenster und 300 Dokumente im gleichen 4-Jahresfenster. Wie groß ist der CiteScore? Lösung: CiteScore = 1200 / 300 = 4.0 Bewertung: 1 Punkt: korrekter Wert. 1 Punkt: korrekte Rechnung. M05-L2-APPLY: Anwendung: Du vergleichst zwei Journals im selben Fach. Journal A: hoher JIF, niedriger SJR. Journal B: niedriger JIF, hoher SJR. Nenne 2 plausible Interpretationen (ohne zu werten). Lösung (Beispiele): (1) Journal B erhält weniger Zitationen im Durchschnitt, aber Zitationen aus sehr „prestigeträchtigen“ Journals (SJR-Gewichtung). (2) Journal A hat höhere durchschnittliche Zitationsraten, aber die zitierenden Quellen sind im SJR-Netzwerk weniger prestigeträchtig oder die Netzwerkstruktur wirkt anders. Bewertung: 1 Punkt: mindestens 1 plausible Interpretation. 1 Punkt: zweite plausible Interpretation ohne Qualitätsbehauptung. Level 3 M05-L3-CASE: Mini-Case (Policy): Ein Departement will eine Regel „Nur Publikationen in Journals mit JIF>5 zählen“. Schreibe eine kurze Gegenempfehlung: (a) 4 Risiken dieser Regel, (b) 3 bessere Alternativen (Metrik-Set + qualitative Kriterien). Deliverable: 4 Risiken + 3 Alternativen (Bulletpoints). Lösungsskizze: Risiken: Feldbias, Artikel-vs-Journal-Fehlschluss, Fehlanreize/Gaming, Ausschluss lokal relevanter Forschung, Benachteiligung junger Felder/SSH. Alternativen: Metrik-Set innerhalb Fachgruppen, Kombination mit Scope/Fit + Peer Review + Open Science/Transparenzsignalen, narrative CV/qualitative Begutachtung. Bewertung: Risiken sind methodisch und governance-tauglich (nicht nur moralisch): 3 Punkte. Alternativen sind praktikabel und mehrdimensional: 2 Punkte. Responsible-Metrics-Prinzipien klar sichtbar: 2 Punkte. Quellen & weiterführende Links Clarivate: The Clarivate Impact Factor (Definition/Überblick): https://clarivate.com/academia-government/essays/impact-factor/ Clarivate Support: Document Types Included in the Impact Factor Calculation: https://support.clarivate.com/ScientificandAcademicResearch/s/article/Journal-Citation-Reports-Document-Types-Included-in-the-Impact-Factor-Calculation?language=en_US Journal Citation Reports (Clarivate): https://clarivate.com/academia-government/scientific-and-academic-research/research-funding-analytics/journal-citation-reports/ Elsevier: CiteScore Hub (Formel/4-Jahresfenster): https://www.elsevier.com/promotions/citescore-hub-explore-journal-rankings-track-trends-compare-performance-predict-directions Elsevier/Scopus: CiteScore metrics: https://www.elsevier.com/products/scopus/metrics/citescore CWTS Journal Indicators: Methodology (SNIP): https://www.journalindicators.com/methodology Waltman et al. (2012). Some modifications to the SNIP journal impact indicator (CWTS Working Paper PDF): https://www.cwts.nl/pdf/CWTS-WP-2012-011.pdf SCImagoJR: SJR Methodology: https://www.scimagojr.com/methodology.php DORA: Read the Declaration: https://sfdora.org/read/ Leiden Manifesto (Website, 10 Prinzipien): https://www.leidenmanifesto.org/ Leiden Manifesto (Nature, 2015) – Originalpublikation: https://www.nature.com/articles/520429a"
    },
    {
      "url": "modul6.html",
      "title": "Modul 6: Feld- & Zeit-Normalisierung: CNCI, FWCI, MNCS, RCR und Top-10%-Indikatoren",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 6: Feld- & Zeit-Normalisierung: CNCI, FWCI, MNCS, RCR und Top-10%-Indikatoren Modul 6: Feld- & Zeit-Normalisierung: CNCI, FWCI, MNCS, RCR und Top-10%-Indikatoren Lernziele Erklären, warum Feld- und Zeit-Normalisierung nötig ist (und was ohne Normalisierung schiefgeht). Die Logik von expected citations, NCS und Aggregationen (MNCS, Percentile/Top-x%) verstehen. CNCI, FWCI und RCR als unterschiedliche Normalisierungsansätze unterscheiden und korrekt interpretieren. Counting-Methoden (full vs. fractional) als relevanten Design-Entscheid für feld-normalisierte Analysen einordnen. Feld-normalisierte Kennzahlen transparent berichten (Quelle, Referenzset, Fenster, Dokumenttypen, Regeln, Limitationen). Kurz erklärt Feld- und zeit-normalisierte Kennzahlen beantworten die Frage: „Wie wurde eine Publikation (oder ein Set) zitiert – im Vergleich zu dem, was für ähnliche Publikationen erwartbar ist?“ Ohne Normalisierung sind Vergleiche zwischen Feldern oder zwischen älteren und jüngeren Publikationen oft unfair. Normalisierung ist daher ein Kerninstrument für Monitoring und Benchmarking – aber nur, wenn Datenbasis, Referenzset und Regeln transparent dokumentiert sind. Begriffe Expected citations (Erwartungswert): Erwartete Zitierungen für eine Publikation basierend auf einem Referenzset (z. B. gleiche Fachkategorie, Publikationsjahr, Dokumenttyp). NCS (Normalized Citation Score): NCS = tatsächliche Zitierungen / erwartete Zitierungen. Werte >1 bedeuten über dem Erwartungswert, MNCS (Mean Normalized Citation Score): Durchschnitt der NCS-Werte einer Publikationsmenge (z. B. Institut/Uni). Durchschnitt = 1 entspricht Referenzset/„Weltmittel“. CNCI (Category Normalized Citation Impact): Clarivate/InCites: tatsächliche Zitierungen dividiert durch erwartete Zitierungen für gleiche Fachkategorie(n), Jahr und Dokumenttyp. FWCI (Field-Weighted Citation Impact): Elsevier/SciVal: Zitierungen relativ zu erwarteten Zitierungen für „ähnliche“ Publikationen (Feld, Jahr, Dokumenttyp). RCR (Relative Citation Ratio): NIH/iCite: feld- und zeit-normalisierte Zitationsrate auf Artikelebene; Feld wird dynamisch über Co-Citation-Network definiert. Top-x%-Indikator (z. B. PP(top 10%)): Anteil der Publikationen, die im Vergleich zu ähnlichen Publikationen (gleiches Feld & Jahr) zu den top x% am häufigsten zitierten gehören. Referenzset: Die Vergleichsmenge, aus der erwartete Zitierungen/Percentiles abgeleitet werden (z. B. gesamte Datenbank, spezifische Felder, Dokumenttypen). Full counting: Jede Publikation zählt vollständig für jede beteiligte Einheit (z. B. jedes Institut erhält 1.0). Fractional counting: Publikationen werden anteilig verteilt (z. B. nach Autor:innen oder Adressen), um Verzerrungen in Aggregationen zu reduzieren. Vertiefung: Drei Normalisierungsfamilien 1) Kategorie-/Feldbasierte Expected-Citation-Modelle (CNCI/FWCI, MNCS/NCS) Du definierst „ähnlich“ über Fachkategorie(n), Jahr und Dokumenttyp. Dann: NCS = actual / expected. Aggregationen: MNCS (Mittel der NCS), oder Summen/Anteile. 2) Dynamische Felder über Zitationsnetzwerke (RCR) Feld wird pro Artikel aus dem Co-Citation-Network abgeleitet. Ergebnis ist artikelbasiert und zeit-normalisiert. 3) Percentile-/Top-x%-Indikatoren (PP(top 10%)) Statt Durchschnittszitationen nutzt du die Frage: „Gehört die Publikation zu den top x% in Feld & Jahr?“ Vorteil: robust gegen extreme Ausreißer; gut für institutionelle Reports. Wichtig: Counting-Methoden Bei institutionellen Vergleichen beeinflusst full vs. fractional counting die Feld-Normalisierung und kann systematische Verzerrungen erzeugen. Für feld-normalisierte Studien ist die Counting-Entscheidung deshalb methodisch relevant (nicht nur „Reporting-Style“). Typische Fehlinterpretationen „Normalisiert = perfekt fair.“ Normalisierung reduziert Verzerrungen, ersetzt aber nicht Kontext (Feldabgrenzung, Dokumenttypen, Datenqualität, Output-Mix). „CNCI/FWCI/RCR messen das Gleiche.“ Sie nutzen unterschiedliche Referenzsets und Felddefinitionen (Kategorien vs. dynamisches Netzwerk). Ergebnisse sind nicht 1:1 austauschbar. „Ein Wert von 2.0 heißt: doppelt so gut.“ Es heißt: doppelt so hoch wie erwartet zitiert (bezogen auf das Referenzset). Es ist kein Qualitätsurteil. „Counting ist egal.“ Bei multi-affiliierten Publikationen kann full counting Aggregationen verzerren; fractional counting kann fairere Vergleiche ermöglichen (je nach Ziel). Praxis-Workflow: Feld-normalisierten Impact berichten Fragestellung klären: Artikel-/Autor:innen-/Institutslevel? Monitoring oder Evaluation? Datenquelle + Referenzset festlegen (und im Report sichtbar machen). „Ähnlichkeit“ definieren: Feld/Kategorien, Jahr, Dokumenttyp; Zitationsfenster festlegen. Counting-Regel festlegen (full vs. fractional) – besonders bei institutionellen Vergleichen. Kennzahl wählen: (a) NCS/MNCS, (b) CNCI/FWCI, (c) PP(top 10%), (d) RCR (artikelbasiert). Plausibilisierung: Ausreißer, Dokumenttypen-Mix, Feldzuordnung, Dubletten/Disambiguierung. Report: Methodik-Kasten + Limitationen + Responsible-Metrics-Hinweis. Responsible Metrics: Normalisierte Kennzahlen Immer angeben: Datenquelle, Referenzset, Jahr(e), Dokumenttypen, Zitationsfenster, Counting-Regel. Vergleiche nur innerhalb sinnvoller Gruppen (z. B. Karrierephase, Feldcluster, Output-Mix). Top-x%-Indikatoren sind oft robuster für Institutionen als reine Mittelwerte (Ausreißer-Effekt). Bei Entscheidungen mit Konsequenzen: Normalisierte Kennzahlen nur zusammen mit qualitativen Verfahren nutzen. Übungen Level 1 M06-L1-Q1: Was bedeutet ein normalisierter Wert von 1.0 (z. B. CNCI/FWCI/MNCS) am häufigsten? Genau 1 Zitation. Zitationen entsprechen dem Erwartungswert für ähnliche Publikationen im Referenzset. Die Publikation ist im Top-1%. Lösung: Zitationen entsprechen dem Erwartungswert für ähnliche Publikationen im Referenzset. M06-L1-Q2: Welche Kennzahl ist ein Percentile-/Top-x%-Indikator? MNCS PP(top 10%) NCS Lösung: PP(top 10%) M06-L1-Q3: Welche Aussage passt am besten zu RCR? RCR normalisiert über starre Fachkategorien im Journal-Set. RCR definiert Felder dynamisch über Co-Citation-Networks und ist artikelbasiert. RCR ist eine Journalmetrik wie der JIF. Lösung: RCR definiert Felder dynamisch über Co-Citation-Networks und ist artikelbasiert. Level 2 M06-L2-CALC-1: Berechne NCS: Eine Publikation hat 15 Zitationen. Erwartungswert (expected) für gleiche Kategorie, Jahr und Dokumenttyp ist 10. Wie groß ist NCS? Lösung: NCS = 15 / 10 = 1.5 Bewertung: 1 Punkt: korrekte Rechnung und Ergebnis. M06-L2-CALC-2: Berechne MNCS: Vier Publikationen haben NCS-Werte 0.8, 1.2, 2.0, 1.0. Wie groß ist MNCS? Lösung: MNCS = (0.8 + 1.2 + 2.0 + 1.0) / 4 = 1.25 Bewertung: 1 Punkt: korrekter Wert. 1 Punkt: korrekte Rechnung. M06-L2-CALC-3: PP(top 10%): In einem Set von 50 Publikationen sind 9 im Top-10% ihrer Feld&Jahr-Referenzsets. Wie groß ist PP(top 10%)? Lösung: PP(top 10%) = 9 / 50 = 0.18 (= 18%) Bewertung: 1 Punkt: korrekter Anteil. 1 Punkt: korrekte Prozentangabe. M06-L2-INTERPRET: Interpretation: Ein Institut hat MNCS = 1.6 und PP(top 10%) = 14%. Nenne 2 vorsichtige Interpretationen (ohne Qualitätsurteil). Lösung: Beispiele: (1) Die Publikationen werden im Mittel häufiger zitiert als im jeweiligen Feld&Jahr erwartet (MNCS>1). (2) Der Anteil sehr hochzitierter Publikationen liegt über der „Top-10%-Baseline“ von 10% (14% statt 10%), was auf überdurchschnittliche Sichtbarkeit/Resonanz hindeuten kann – abhängig von Datenbasis, Output-Mix und Counting-Regeln. Bewertung: 1 Punkt: MNCS defensibel erklärt. 1 Punkt: PP(top10) defensibel erklärt. Level 3 M06-L3-CASE: Mini-Case: Du sollst zwei Departemente vergleichen (2020–2024). Liefere (a) einen Methodik-Kasten (Quelle, Referenzset, Dokumenttypen, Fenster, Counting-Regel) und (b) 6 Limitationen/Warnhinweise, die speziell für feld-normalisierte Vergleiche relevant sind. Deliverable: Methodik-Kasten + 6 Limitationen (Bulletpoints). Lösungsrahmen: Methodik: Datenquelle(n), Abfragedatum, Zeitraum 2020–2024, Dokumenttypen (Article/Review getrennt), Zitationsfenster, Referenzset (Feld&Jahr), Kennzahlen (z. B. MNCS + PP(top10)), Counting (full vs. fractional). Limitationen: Feldabgrenzung/Kategorien, Output-Mix (Reviews), Datenqualitätsprobleme (Disambiguierung/Affiliation), Coverage-Bias der Quelle, Zeitdynamiken (Fenster), counting-bedingte Verzerrungen, Ausreißer/Skewness, Interdisziplinarität (Mehrfachkategorien). Bewertung: Methodik ist vollständig (Quelle+Referenzset+Fenster+Dokumenttypen+Counting): 3 Punkte. Limitationen sind konkret und feld-normalisierungsspezifisch: 3 Punkte. Responsible-Metrics-Transparenz ist sichtbar: 2 Punkte. Quellen https://incites.zendesk.com/hc/en-gb/articles/25087312115601-Category-Normalized-Citation-Impact-CNCI https://supportcontent.elsevier.com/RightNow%20Next%20Gen/SciVal/ACAD_RL_ElsevierResearchMetricsBook_WEB.pdf https://traditional.leidenranking.com/information/indicators https://pmc.ncbi.nlm.nih.gov/articles/PMC3081055/ https://arxiv.org/abs/1501.04431 https://support.icite.nih.gov/hc/en-us/articles/9105587565851-Relative-Citation-Ratio-RCR https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002541 Weiterführende Literatur https://www.cwts.nl/blog?article=n-q2w274 https://www.cwts.nl/TvR/documents/TonvR%281%29.pdf"
    },
    {
      "url": "modul7.html",
      "title": "Modul 7: Collaboration & Co-Authorship: Indikatoren, Fractional Counting und Co-Authorship Networks",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 7: Collaboration & Co-Authorship: Indikatoren, Fractional Counting und Co-Authorship Networks Modul 7: Collaboration & Co-Authorship: Indikatoren, Fractional Counting und Co-Authorship Networks Lernziele Co-Authorship als Messkonzept erklären (und seine Grenzen benennen). Wichtige Kollaborationsindikatoren korrekt berechnen (domestic/international/industry; Anteile/Quoten). Full vs. fractional counting unterscheiden und die Auswirkungen auf Output-, Impact- und Netzwerk-Analysen verstehen. Hyperauthorship als Sonderfall erkennen und methodisch sauber behandeln. Einen Collaboration-Report transparent dokumentieren (Datenquelle, Regeln, Counting, Limitationen). Kurz erklärt Kollaboration wird in der Bibliometrie häufig über Co-Authorship gemessen. Das ist praktisch, weil es aus Metadaten ableitbar ist – aber nicht identisch mit „echter Zusammenarbeit“. Für institutionelle Vergleiche ist entscheidend, wie du zählst: Full counting erhöht Output bei Kooperationen; fractional counting verteilt Kredit und reduziert Mehrfachzählungen. In Netzwerken (Co-Authorship Networks) wirken Counting-Regeln und Hyperauthorship besonders stark – daher braucht es klare Methodik und transparente Berichte. Begriffe Co-authorship: Gemeinsame Autorschaft als Proxy für Zusammenarbeit; gemessen über Publikationen mit mehreren Autor:innen/Organisationen. International collaboration: Publikationen mit Co-Autor:innen bzw. Institutionen aus mehr als einem Land. Domestic collaboration: Publikationen mit mehreren Institutionen im selben Land (keine internationalen Co-Autor:innen). Industry collaboration: Publikationen mit mindestens einer Corporate-/Industry-Affiliation (Definition je Plattform/Unification-Status). Collaboration rate: Anteil kollaborativer Publikationen (z. B. % International Collab = International collab papers / total papers). Full counting: Jede beteiligte Einheit erhält volle Zählung (1.0) pro Publikation; führt bei Kooperationen zu Mehrfachzählungen. Fractional counting: Kredit wird auf Einheiten aufgeteilt (z. B. nach Autor:innen, Adressen oder Organisationen), um Mehrfachzählungen zu reduzieren. Binary collaboration counting: Pro Einheit gilt: kollaboriert (1) oder nicht (0); eine Publikation zählt maximal einmal pro Einheit/Pairing. Co-authorship network: Netzwerk aus Knoten (z. B. Autor:innen/Orgs/Länder) und Kanten (Co-Authorship-Links), meist gewichtet nach Anzahl gemeinsamer Publikationen. Link strength: Stärke einer Kante im Netzwerk (z. B. Anzahl gemeinsamer Publikationen; ggf. fractionally gewichtet). Hyperauthorship: Publikationen mit ungewöhnlich vielen Autor:innen (z. B. Konsortien), die Kennzahlen und Netzwerke stark verzerren können. Vertiefung: Indikatoren – was wird gezählt? Typische Indikatoren sind Anteile kollaborativer Publikationen (z. B. % International Collaboration), domestic vs. international, sowie Industry Collaboration. Wichtig: Plattformen definieren diese Kategorien unterschiedlich (z. B. über Adressen/Organisationen, Unification-Status, Corporate-Typen). Zusätzlich gibt es zwei verbreitete Zähllogiken: (a) Anteile/Quoten auf Basis von Publikationsmengen und (b) Paar-bezogene Kollaborationszählungen (z. B. Land A–B), oft als binär gezählt (kollaboriert ja/nein), um Doppelzählung zu reduzieren. Vertiefung: Counting – full vs. fractional (und warum das nicht nur „Reporting-Style“ ist) Full counting: Jede beteiligte Einheit bekommt 1.0 pro Publikation. Das macht Kooperationen „größer“ und kann Rankings/Benchmarks verzerren. Fractional counting: Kredit wird geteilt (z. B. 1/N nach Autor:innen oder anteilig nach Organisationen/Adressen). Dadurch wird Output bei großen Kooperationen nicht automatisch überproportional aufgeblasen. Für viele Zwecke (z. B. institutionelle Vergleiche) ist fractional counting methodisch attraktiver, weil es systematische Verzerrungen durch unterschiedliche Kooperationskulturen reduziert. Vertiefung: Co-Authorship Networks – was zeigen sie (und was nicht)? Ein Co-Authorship Network hat Knoten (z. B. Autor:innen, Organisationen, Länder) und Kanten (gemeinsame Publikationen). Die Kanten können gewichtet sein (z. B. Anzahl gemeinsamer Publikationen) und lassen sich mit Tools wie VOSviewer visualisieren. Wichtig: Hyperauthored papers können zentrale Netzwerkmetriken stark aufblasen. Zwei typische Gegenmaßnahmen sind (a) definierte Cutoffs/Filter und/oder (b) fractional weighting. Typische Fallen & Fehlinterpretationen „Co-Authorship = echte Zusammenarbeit.“ Co-Authorship ist ein Proxy: Er misst gemeinsame Autorschaft, nicht zwingend Intensität, Rollen oder tatsächliche Arbeitsanteile. „Mehr International Collab heißt automatisch bessere Forschung.“ Internationalität kann Sichtbarkeit erhöhen, ist aber feld-, projekt- und förderlogikabhängig; es ist kein Qualitätsbeweis. „Full und fractional liefern ähnliche Ergebnisse.“ Gerade bei großen Kooperationen unterscheiden sich Output, Impact und Netzwerkstrukturen stark. „Netzwerkzentralität zeigt die wichtigsten Forschenden.“ Zentralität ist sensibel gegenüber Hyperauthorship, Datenbereinigung und Zählregeln; interpretierbar nur mit Kontext. Praxis-Workflow: Collaboration-Analyse sauber aufsetzen Zweck klären: Monitoring, Strategie, Reporting oder Evaluation? (High-stakes -> besonders vorsichtig). Einheit definieren: Autor:in, Organisation, Land, Departement; und Ebenen trennen (nicht mischen). Datenquelle + Exportregeln dokumentieren (Coverage, Dokumenttypen, Zeitraum, Stichtag). Kollaborationsdefinition festlegen: international/domestic/industry; Regeln zu Multi-Affiliations. Counting-Regel festlegen: full vs. fractional (und welche Fractional-Variante: nach Autor:innen, Adressen, Organisationen). Hyperauthorship-Policy: Cutoff/Filter oder fractional weighting (und im Report begründen). Netzwerk-Analyse (optional): Knotenebene wählen, Linkdefinition (co-authored papers), Gewichtung, Thresholds. Plausibilisieren: Disambiguierung (Orgs), Dubletten, Konsortien, Multi-Affiliations, Feldmix. Report schreiben: Methodik-Kasten + Limitationen + Responsible-Metrics-Hinweis. Tool-Lab: Co-Authorship Network in VOSviewer (15 Minuten) Export: Aus WoS/Scopus/Dimensions/Lens/OpenAlex eine Publikationsliste exportieren (inkl. Autor:innen + Affiliations). VOSviewer: Create Map -> Bibliographic data -> Co-authorship. Item type wählen: Authors ODER Organizations ODER Countries (nicht mischen). Counting: Full vs. Fractional (falls angeboten) bzw. Gewichtung/Thresholds so einstellen, dass die Karte lesbar bleibt. Cleaning: Thesaurus/Name-Variants (z. B. Organisationsnamen vereinheitlichen). Interpretation: Cluster = dichte Kollaborationsgruppen; Overlay/Density nutzen, aber mit Vorsicht (Hyperauthorship beachten). Responsible Metrics: Kollaboration Immer dokumentieren: Definition von „international/domestic/industry“, Datenquelle, Zeitraum, Counting-Regel, Hyperauthorship-Policy. Für Vergleiche: gleiche Regeln für alle Einheiten; Feldmix und Output-Mix explizit benennen. Keine mechanischen Schlüsse: Kollaboration ist Strategie-/Strukturindikator, nicht automatisch Qualitätsindikator. Netzwerkmetriken nur als explorative Evidenz nutzen, nicht als Ranking-Automat. Übungen Level 1 M07-L1-Q1: Was ist der wichtigste Unterschied zwischen full und fractional counting? Full counting ist nur für Zitationen, fractional nur für Publikationen. Full counting gibt jeder Einheit volle Zählung; fractional verteilt Kredit auf Einheiten. Fractional counting zählt nur internationale Kooperationen. Lösung: Full counting gibt jeder Einheit volle Zählung; fractional verteilt Kredit auf Einheiten. M07-L1-Q2: Welche Aussage ist am vorsichtigsten? Internationale Kollaboration beweist hohe Qualität. Internationale Kollaboration ist ein Strukturindikator, der kontextabhängig ist. Ohne Kollaboration ist Forschung weniger wertvoll. Lösung: Internationale Kollaboration ist ein Strukturindikator, der kontextabhängig ist. M07-L1-Q3: Hyperauthorship ist problematisch, weil … es Zitationen komplett verhindert. es Kennzahlen/Netzwerkmetriken stark verzerren kann. es nur in den Geisteswissenschaften vorkommt. Lösung: es Kennzahlen/Netzwerkmetriken stark verzerren kann. Level 2 M07-L2-CALC-1: Berechne Kollaborationsraten (Publikations-Set mit 10 Publikationen): 6 sind multi-institutionell im selben Land (domestic), 2 sind multi-country (international), 2 sind single-institution. Wie groß sind % domestic, % international, % no-collaboration? Lösung: % domestic = 6/10 = 60%; % international = 2/10 = 20%; % no-collaboration = 2/10 = 20% Bewertung: 1 Punkt: alle drei korrekt. M07-L2-CALC-2: Full vs. Fractional (vereinfachtes Autorenmodell): Eine Publikation hat 5 Autor:innen: 2 von Uni A, 3 von Uni B. Wie viel Output bekommt Uni A (a) bei full counting, (b) bei fractional counting nach Autor:innen? Lösung: (a) Full: 1.0; (b) Fractional: 2/5 = 0.4 Bewertung: 1 Punkt: full korrekt. 1 Punkt: fractional korrekt. M07-L2-NET-1: Netzwerk-Kante (Publikationszählung): Autor A und B haben 3 gemeinsame Papers, A und C haben 1 gemeinsames Paper. Welche Link Strength ist stärker? Um wie viel? Lösung: A–B ist stärker (3 vs. 1), also um Faktor 3 bzw. Differenz 2. Bewertung: 1 Punkt: korrekt. M07-L2-INTERPRET: Interpretation: Ein Departement hat % international = 45% (5 Jahre), aber der Wert sprang im letzten Jahr auf 70%. Nenne 2 plausible, nicht-wertende Erklärungen. Lösung: Beispiele: (1) Start/Ende eines großen, internationalen Projekts oder Konsortiums. (2) Änderung in Publikationsmix (z. B. mehr Multi-Center-Studien) oder bessere Affiliation-Erfassung/Unification in der Datenquelle. Bewertung: 1 Punkt: 1 plausible Erklärung. 1 Punkt: 2. plausible Erklärung. Level 3 M07-L3-CASE: Mini-Case: Du erstellst einen Uni-Report „Collaboration 2021–2025“. Liefere (a) einen Methodik-Kasten (Datenquelle, Zeitraum, Dokumenttypen, Definition international/domestic/industry, Counting, Hyperauthorship-Policy) und (b) 8 Limitationen/Warnhinweise. Deliverable: Methodik-Kasten + 8 Limitationen (Bulletpoints). Lösungsrahmen: Methodik: Quelle+Stichtag; Zeitraum; Dokumenttypen; Kollaborationsdefinitionen; Counting (full vs fractional + Variante); Hyperauthorship (Cutoff oder fractional weighting); Disambiguierung/Unification. Limitationen: Coverage-Bias; Feldmix; Multi-affiliations; Unification-Fehler; Konsortien/Hyperauthorship; Namensvarianten; Zitations-/Output-Fenster-Effekte; Vergleichsgruppenwahl; Unterschiede in Kollaborationskultur. Bewertung: Methodik vollständig und nachvollziehbar: 4 Punkte. Limitationen konkret (daten- & methodenspezifisch): 3 Punkte. Responsible-Metrics-Logik sichtbar: 2 Punkte. Quellen https://traditional.leidenranking.com/information/indicators https://incites.zendesk.com/hc/en-gb/articles/25087437743889-Collaboration-Indicators https://oecd.ai/en/partner-data-methodological-note https://www.sciencedirect.com/science/article/abs/pii/S1751157716302036 https://ideas.repec.org/a/eee/infome/v9y2015i4p872-894.html https://www.vosviewer.com/documentation/Manual_VOSviewer_1.6.18.pdf https://direct.mit.edu/qss/article/5/3/613/120830/Hyperauthored-papers-disproportionately-amplify Weiterführende Literatur https://www.vosviewer.com/download/f-x2.pdf https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253397"
    },
    {
      "url": "modul8.html",
      "title": "Modul 8: Alternative Metriken & Aufmerksamkeit: Altmetrics, Usage, Policy Mentions – sinnvoll nutzen",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 8: Alternative Metriken & Aufmerksamkeit: Altmetrics, Usage, Policy Mentions – sinnvoll nutzen Modul 8: Alternative Metriken & Aufmerksamkeit: Altmetrics, Usage, Policy Mentions – sinnvoll nutzen Lernziele Erklären, was Altmetrics messen (und was nicht): Aufmerksamkeit, Nutzung, Saves, Mentions – nicht automatisch Qualität oder gesellschaftlicher Impact. Altmetric Attention Score, PlumX-Kategorien und COUNTER-Usage sauber unterscheiden und korrekt interpretieren. Policy Mentions/Policy Citations als Impact-Signal einordnen (Methodik, Limitationen, Kontext). Ein transparentes Reporting-Template für alternative Metriken erstellen (Quelle, Zeitraum, Abfragedatum, Regeln, Coverage, Bots/Spam). Responsible-Metrics-Leitplanken auf Altmetrics anwenden (klar, transparent, kontextualisiert, fair). Kurz erklärt Altmetrics und Usage-Metriken ergänzen Zitationsmetriken: Sie zeigen, ob Forschung gelesen, gespeichert, diskutiert oder in Policy-Kontexten aufgegriffen wird. Das ist besonders nützlich für frühe Signale (z. B. „Captures“), für Outputs außerhalb klassischer Artikel (Datasets, Preprints) und für Kommunikation. Aber: Aufmerksamkeit ist nicht gleich Qualität. Gute Praxis heißt: Datenquelle, Zeitraum, Regeln, Coverage und mögliche Verzerrungen immer mitliefern – und Altmetrics nie als alleinigen Bewertungsmaßstab verwenden. Vertiefung: Altmetrics-Familien – 4 Signaltypen (mit typischen Use-Cases) 1) Social & Media Attention (News, Blogs, Social) Use-case: Kommunikationsreichweite, Themenresonanz, „public attention“. Risiko: Sensations-/Kontroversen-Bias, Bots/Amplification. 2) Usage (Views/Downloads) Use-case: Nutzung/Interesse an Content (insb. OA, Repositories, Lernressourcen). Good practice: Standardisierte COUNTER-Reports nutzen; klare Definitionen (View vs Download). 3) Captures / Saves (z. B. Mendeley Readers) Use-case: Frühes Signal von akademischem Interesse; kann Zitationen zeitlich vorauslaufen. Risiko: Plattformabhängigkeit, Fachbias. 4) Mentions in Wissens-/Policy-Kontexten (Wikipedia, Policy documents) Use-case: „Evidence uptake“ in Wissens- und Entscheidungskontexten. Risiko: Coverage/Indexing, heterogene Dokumenttypen, Zitierpraktiken. Vertiefung: Altmetric Attention Score – was steckt dahinter? Der Altmetric Attention Score ist eine gewichtete Zählung von Aufmerksamkeit. Gewichte hängen von Quellentypen ab (z. B. News höher als einzelne Social-Mentions). Wichtig: Score = Aufmerksamkeit in erfassten Quellen, nicht „Impact“. Für Reporting: immer Score + Quellenaufschlüsselung + Zeitraum + auffällige Peaks (z. B. Medienereignis) dokumentieren. Vertiefung: PlumX – 5 Kategorien als saubere Struktur fürs Teaching PlumX ordnet Metriken in fünf Kategorien: Citations, Usage, Captures, Mentions, Social Media . Das ist didaktisch stark: Lernende können bei jedem Output (Artikel, Dataset, Software, Buchkapitel) systematisch fragen, welche Kategorie überhaupt sinnvoll ist – und welche Verzerrungen zu erwarten sind. Vertiefung: Usage (COUNTER) – warum Standardisierung zählt Nutzungsdaten sind nur dann vergleichbar, wenn sie nach einem Standard erzeugt werden. COUNTER liefert Definitionen, Reportstrukturen und Compliance-Regeln. Für Bibliotheken und Uni-Reporting ist das der solide Weg, um Views/Downloads nicht willkürlich zu interpretieren. Vertiefung: Policy Mentions – Impact-Signal mit Methodikpflicht Policy-Mentions können zeigen, dass Forschung in Entscheidungs- oder Guidelines-Kontexten aufgegriffen wird. Tools wie Overton indexieren Policy-Dokumente und verknüpfen dort referenzierte Forschung. Für seriöse Nutzung braucht es: Definition des Policy-Korpus, Transparenz über Abdeckung, Duplikate, Dokumenttypen (Guidelines vs. Whitepaper) und qualitative Plausibilisierung (welche Rolle spielt die Quelle im Dokument?). Typische Fehlinterpretationen „Altmetrics zeigen gesellschaftlichen Impact.“ Altmetrics zeigen oft Aufmerksamkeit oder Nutzung; Impact kann dazugehören, muss aber kontextualisiert und zusätzlich belegt werden. „Ein hoher Attention Score = hohe wissenschaftliche Qualität.“ Der Score misst Aufmerksamkeit in erfassten Quellen. Er kann durch Kontroversen, Medienzyklen oder Bots verzerrt sein. „Usage ist objektiver als Zitationen.“ Usage hängt von Plattform, OA-Status, UI/SEO, Kursnutzung und Messstandard ab. Ohne COUNTER/Regeln ist Vergleich riskant. „Policy mentions sind ein harter Beweis für Wirkung.“ Policy-Zitationen sind ein starkes Signal, aber nur mit Korpus-/Dokumenttyp-Transparenz und Kontextanalyse sinnvoll. Praxis-Workflow: Alternative Metriken report-tauglich machen Ziel klären: Kommunikation, Monitoring, Impact-Storytelling, Evaluation? (High-stakes => besonders restriktiv). Output-Typ bestimmen: Artikel, Preprint, Dataset, Software, Buchkapitel (nicht jede Metrik passt überall). Quelle(n) wählen und dokumentieren: Altmetric, PlumX, COUNTER-Reports, Overton etc. Parameter fixieren: Abfragedatum, Zeitraum, Identifikatoren (DOI, PMID, ISBN), Versionen (Preprint vs Published). Aufschlüsselung erfassen: Welche Quellen/Events treiben den Wert? (News vs Social vs Policy). Qualitäts-/Risiko-Checks: Bots/Spam, Medienereignis, Duplikate, Fehlzuordnung (Identifier). Report: Methodik-Kasten + Ergebnis (mit Breakdown) + Limitationen + verantwortungsvolle Interpretation. Tool-Lab (optional): Offene Event-Daten als Übungsdatensatz Nutze Crossref Event Data als Beispiel für offene Event-Logs rund um DOIs (API-basiert). Ziehe eine kleine Stichprobe (z. B. 50 Events) und klassifiziere sie nach Signaltyp (Attention/Usage/Capture/Mention). Diskutiere Coverage: Welche Plattformen sind drin, welche fehlen? Welche Events sind interpretierbar, welche nicht? Responsible Metrics: Altmetrics & Usage Altmetrics/Usage nie als alleinige Bewertungsbasis nutzen – besonders nicht für Personenentscheidungen. Immer transparent: Quelle, Zeitraum, Abfragedatum, Identifier/Version, Coverage, Regeln (z. B. Gewichtung). Kontext liefern: warum gab es Aufmerksamkeit (Medienereignis, Pressemitteilung, Kontroverse)? Vergleiche nur mit kompatiblen Datengrundlagen (z. B. COUNTER-konforme Reports). Wenn Impact behauptet wird: zusätzliche Evidenz (qualitative Nachweise, Testimonials, konkrete Uptake-Beispiele). Übungen Level 1 M08-L1-Q1: Welche Aussage ist am defensibelsten? Altmetrics messen wissenschaftliche Qualität. Altmetrics messen Aufmerksamkeit/Nutzung und müssen kontextualisiert werden. Altmetrics ersetzen Zitationsmetriken vollständig. Lösung: Altmetrics messen Aufmerksamkeit/Nutzung und müssen kontextualisiert werden. M08-L1-Q2: Welche Kategorie gehört bei PlumX zu \"Captures\"? Downloads Mendeley Readers Journal Impact Factor Lösung: Mendeley Readers M08-L1-Q3: Wozu dient COUNTER primär? Zur Berechnung des h-Index. Zur Standardisierung von Nutzungsstatistiken (Usage) für Online-Content. Zur Berechnung von Journalmetriken wie SNIP. Lösung: Zur Standardisierung von Nutzungsstatistiken (Usage) für Online-Content. Level 2 M08-L2-CLASSIFY-1: Ordne die Ereignisse einem Signaltyp zu (Attention / Usage / Capture / Mention-Policy): Ein Artikel wird in einem großen Online-News-Portal besprochen. Ein Repository meldet 800 Downloads eines Datasets (COUNTER-konform). 120 Mendeley Readers speichern eine Publikation. Eine Leitlinie (Guideline) einer Behörde zitiert den Artikel. Lösung: News-Portal -> Attention; Repository-Downloads -> Usage; Mendeley Readers -> Capture; Behördenleitlinie -> Mention-Policy. Bewertung: 2 Punkte: alle 4 korrekt; 1 Punkt: 3 korrekt. M08-L2-REPORT-1: Formuliere einen Methodik-Kasten (3–5 Sätze) für einen Altmetrics-Report zu einem Artikel: Quelle, Abfragedatum, Zeitraum, Score + Quellenaufschlüsselung, Limitation (Aufmerksamkeit ≠ Qualität). Lösungsrahmen: Quelle (Altmetric/PlumX etc.) nennen; Abfragedatum + betrachteter Zeitraum; Ergebnis immer mit Breakdown (News/Blogs/Policy/Social) nennen; Hinweis: Score ist Aufmerksamkeit in erfassten Quellen; nicht Qualität/Impact-Beweis. Bewertung: 1 Punkt: Quelle+Datum. 1 Punkt: Breakdown/Transparenz. 1 Punkt: korrekte Limitation. M08-L2-CHECK-1: Plausibilitätscheck: Ein Paper hat plötzlich 3'000 Tweets in 24 Stunden. Nenne 3 Checks, bevor du das als „große Wirkung“ interpretierst. Lösung: Beispiele: (1) Event-/Peaks prüfen (Pressemitteilung, kontroverse News, Influencer-Post). (2) Bot-/Spam-Indikatoren (auffällige Konten, Wiederholungsmuster, Koordination). (3) Quellenmix anschauen: nur Social oder auch News/Policy? (4) Identifier korrekt? (falsches Paper verlinkt?). Bewertung: 1 Punkt: Bot/Spam-Check. 1 Punkt: Kontext/Peak-Check. 1 Punkt: Quellenmix oder Identifier-Check. Level 3 M08-L3-CASE: Mini-Case: Fakultät will Altmetrics für Leistungsbeurteilung nutzen. Schreibe (a) 6 Risiken und (b) 4 Alternativen, wie Altmetrics verantwortungsvoll eingesetzt werden können (Kommunikation, Impact-Narrative, Monitoring). Deliverable: 6 Risiken + 4 Alternativen (Bulletpoints). Lösungsrahmen: Risiken: Bots/Gaming, Medien-/Kontroversen-Bias, Coverage-Bias, Feldbias, OA/SEO-Effekte, Fehlanreize, Datenschutz/Plattformabhängigkeit. Alternativen: Altmetrics nur als Kommunikationsindikator; narrative Impact-Statements + Evidenz; Policy-Uptake qualitativ prüfen; COUNTER-Usage für Ressourcenplanung; klare Leitlinie (DORA/Leiden/NISO) und Transparenzpflicht. Bewertung: Risiken sind daten- und governance-tauglich (3 Punkte). Alternativen sind praktikabel und verantwortungsvoll (3 Punkte). Responsible-Metrics-Logik sichtbar (Kontext+Transparenz+Fairness) (2 Punkte). Quellen https://zenodo.org/records/12684249 https://www.niso.org/publications/rp-25-2016-altmetrics https://help.altmetric.com/support/solutions/articles/6000233311-how-is-the-altmetric-attention-score-calculated- https://www.altmetric.com/about-us/our-data/donut-and-altmetric-attention-score/ https://www.elsevier.com/insights/metrics/plumx https://cop5.projectcounter.org/_/downloads/en/5.1/pdf/ https://www.crossref.org/documentation/event-data/ https://direct.mit.edu/qss/article/3/3/624/112760/Overton-A-bibliometric-database-of-policy-document https://sfdora.org/resource/guidance-on-the-responsible-use-of-quantitative-indicators-in-research-assessment/ https://www.ucl.ac.uk/research/sites/research/files/responsible_use_of_new_and_alternative_metrics.pdf"
    },
    {
      "url": "modul9.html",
      "title": "Modul 9: Datenqualität & Disambiguierung: Autor:innen, Organisationen, Affiliations, Dubletten – plus reproduzierbare Workflows",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 9: Datenqualität & Disambiguierung: Autor:innen, Organisationen, Affiliations, Dubletten – plus reproduzierbare Workflows Modul 9: Datenqualität & Disambiguierung: Autor:innen, Organisationen, Affiliations, Dubletten – plus reproduzierbare Workflows Lernziele Die wichtigsten Datenqualitäts-Probleme in bibliometrischen Datensätzen erkennen (Name ambiguity, Affiliations, Dubletten, Coverage). Autor:innen- und Organisations-Disambiguierung als eigenes Arbeitsfeld verstehen (warum es nie „perfekt“, aber besser wird). PIDs (ORCID, ROR, DOI) als robuste Anker für saubere Datensätze nutzen. Ein Cleaning- & QA-Protokoll aufsetzen (Checklisten, Stichproben, Edge-Cases, Logging). Einen bibliometrischen Workflow reproduzierbar dokumentieren (Stichtag/Snapshot/Versionierung/Parameter). Kurz erklärt Bibliometrie steht und fällt mit Datenqualität. Schon kleine Zuordnungsfehler (falsche Autor:in, falsche Affiliation, Dublette) können Kennzahlen und Rankings stark verzerren. Disambiguierung ist daher kein „Nachputzen“, sondern ein Kernschritt. Gute Praxis kombiniert PIDs (ORCID/ROR/DOI), klar definierte Regeln (Counting, Zeitfenster, Dokumenttypen), QA-Checks (Stichproben/Edge-Cases) und reproduzierbare Dokumentation (Stichtag, Parameter, Versionierung). Schlüsselbegriffe Data quality dimensions: Typische Dimensionen: Vollständigkeit, Genauigkeit, Konsistenz, Aktualität, Provenienz/Traceability. Name ambiguity: Mehrdeutige Namen (z. B. Initialen, Namensänderungen, Transliteration) führen zu falscher Zuordnung von Publikationen. Author disambiguation: Algorithmische/kuratorische Zuordnung von Werken zu Personen (mit Unsicherheit, Konflikten und iterativer Verbesserung). Organization disambiguation: Vereinheitlichung von Organisationsnamen (Varianten, Sprachen, Fusions-/Reorg-Historie) zu stabilen Einheiten. Affiliation: Zuordnung einer Autor:in zu einer Organisation im Kontext eines bestimmten Works (zeit-/work-spezifisch). Duplicate / near-duplicate: Doppelte oder beinahe doppelte Records (Preprint vs. Version of Record; Importduplikate; Metadatenvarianten). Persistent Identifier (PID): Stabiler, eindeutiger Identifier (z. B. ORCID für Personen, ROR für Organisationen, DOI für Objekte). ORCID iD: Kostenloser, persistenter Personen-Identifier, der Name-Ambiguität reduziert und Beiträge/Outputs verknüpfen hilft. ROR ID: Offener, persistenter Identifier für Forschungseinrichtungen zur Disambiguierung von Organisationsnamen. DOI: Persistenter Identifier für digitale Objekte (z. B. Artikel, Daten, Preprints), inkl. Metadaten-Verknüpfung. Snapshot / Stichtag: Fixierter Stand der Daten (Zeitpunkt), der für Reproduzierbarkeit im Reporting zwingend dokumentiert werden sollte. Vertiefung: Wo Fehler typischerweise entstehen 1) Personen: Namensvarianten, Initialen, Namenswechsel, gleichnamige Personen. 2) Organisationen: Schreibvarianten (DE/EN/FR), Fakultäten/Institute vs. Gesamtuni, Fusionen/Reorganisation. 3) Affiliations im Work-Kontext: Mehrfachaffiliationen, fehlende oder veraltete Angaben. 4) Werke: Preprint vs. Journal-Artikel, Konferenzversionen, doppelte Importe. 5) Datenbasis/Coverage: Nicht alle Felder/Sprachen/Publikationstypen sind gleich gut abgedeckt. Vertiefung: Disambiguierung als Pipeline (statt „manuell irgendwo korrigieren“) Ziel: Aus rohen Metadaten wird ein analysefähiger, auditierbarer Datensatz. Schrittfolge (Minimalstandard): Identifier-Join zuerst (ORCID/ROR/DOI), wo möglich. Normalisierung (Casefolding, Unicode, Trimmen; Standardisierung von Ländern, Institutionssuffixen). Matching-Regeln (exakt, dann fuzzy; immer mit Schwellen/Confidence). Konfliktregeln (z. B. ORCID > Name-Match; ROR > Freitext). Deduplication (DOI/PMID/ISBN; plus Near-Duplicate-Heuristiken). QA : Stichprobe + gezielte Edge-Case-Tests; Logging aller Änderungen. Merksatz: Jede Korrektur braucht eine Regel + einen Audit-Trail. PIDs als robuste Anker (ORCID, ROR, DOI) ORCID Use: Personen eindeutig identifizieren; Profilpflege lohnt sich für Disambiguierung. Best Practices: ORCID in Publikationssystemen und Repositorien erfassen. ORCID in Reports als Primäranker (falls vorhanden) nutzen. Bei Konflikten: ORCID-gestützte Zuordnung bevorzugen. ROR Use: Organisationen konsistent benennen und über Systeme hinweg verbinden. Best Practices: ROR als Master-Lookup für Organisationsnamen und Varianten nutzen. Sub-Units (Departemente/Institute) getrennt modellieren (Mapping-Tabelle). Reorg-Historie dokumentieren (ab wann gilt welche Struktur). DOI Use: Werke stabil identifizieren; erleichtert Dedup und Metadaten-Abgleich (Crossref/DataCite). Best Practices: DOI als primären Work-Key nutzen (wenn vorhanden). Preprint vs. VoR explizit modellieren (Relation/Version). Metadaten-Abgleich via APIs (z. B. Crossref) für Plausibilisierung. QA-Checkliste (für jede Analyse / jeden Report) Stichtag/Snapshot dokumentiert (Datum, Datenquelle, Exportparameter). Identifier-Quote: % Werke mit DOI, % Autor:innen mit ORCID, % Orgs mit ROR. Top-20 Autor:innen/Orgs manuell plausibilisiert (Fehlzuordnungen?). Dublettentest: DOI-Duplikate = 0; Near-Duplicates geprüft (Preprint/VoR). Affiliation-Qualität: fehlende Affiliationsquote; Multi-affiliation-Regel dokumentiert. Edge-Case-Liste: Konsortien/Hyperauthorship; Namenswechsel; transliterierte Namen. Audit-Trail: Welche Regeln/Overrides wurden angewendet? (mit Version) Template: Methodik-Kasten (Copy/Paste für Website & Reports) Datenquelle(n): | Stichtag/Export: | Zeitraum: | Dokumenttypen: | Identifiers: DOI/ORCID/ROR (Prioritäten: ) | Disambiguierung: | Dedup: | Counting: | Limitationen: | Reproduzierbarkeit: . Tool-Lab: Mini-Lab – 30-Minuten Cleaning Sprint (Author + Org) Nimm ein Set von 200 Publikationen (z. B. 2022–2025) aus deiner Datenquelle. Ermittle Identifier-Quoten (DOI/ORCID/ROR) und dokumentiere sie. Wähle 10 häufigste Autor:innen und 10 häufigste Organisationen; prüfe Fehlzuordnungen (Stichprobe 5 Werke pro Einheit). Lege ein Mapping-Sheet an: Org-Varianten -> ROR; Sub-Units -> Canonical Name. Führe Dedup-Check (DOI) und Near-Dup-Check (Titel+Jahr+1. Autor:in) aus. Erstelle am Ende einen Methodik-Kasten + eine Edge-Case-Liste (max. 10 Punkte). Responsible Metrics: Datenqualität zuerst Keine Kennzahl ohne Datenbasis- und Disambiguierungs-Transparenz. Wenn Datenqualität unklar ist: lieber robuste Indikatoren (z. B. Top-x%) und größere Aggregationseinheiten. Immer Missingness berichten (DOI/ORCID/ROR-Quote; Affiliation-Quote). Korrekturen müssen nachvollziehbar sein (Regel + Audit-Trail) – besonders bei Entscheidungen mit Konsequenzen. Übungen Level 1 M09-L1-Q1: Welche Reihenfolge ist als Minimalstandard am sinnvollsten? Fuzzy-Matching zuerst, dann Identifier-Join. Identifier-Join zuerst (ORCID/ROR/DOI), dann Matching für den Rest. Nur manuelle Korrektur, keine Regeln. Lösung: Identifier-Join zuerst (ORCID/ROR/DOI), dann Matching für den Rest. M09-L1-Q2: Warum ist ein Stichtag/Snapshot wichtig? Damit Zahlen größer wirken. Damit Analysen reproduzierbar sind und Änderungen über Zeit erklärbar bleiben. Weil APIs sonst nicht funktionieren. Lösung: Damit Analysen reproduzierbar sind und Änderungen über Zeit erklärbar bleiben. M09-L1-Q3: Welche Aussage ist am defensibelsten? Disambiguierung ist immer 100% korrekt. Disambiguierung ist ein iterativer Prozess mit Unsicherheit, der dokumentiert werden muss. PIDs sind unwichtig, weil Namen reichen. Lösung: Disambiguierung ist ein iterativer Prozess mit Unsicherheit, der dokumentiert werden muss. Level 2 M09-L2-CLEAN-1: Du bekommst 6 Organisationsnamen aus Publikationen. Erstelle eine Mapping-Tabelle (Original -> Canonical -> ROR-ID (falls auffindbar) -> Notes). Datensnippet: University of Lucerne Univ. Luzern Université de Lucerne Dept. of Economics, Univ. of Lucerne Universität Luzern, Wirtschaftswissenschaftliches Institut U. Lucerne Lösungsrahmen: Canonical Name: einheitlich (z. B. University of Lucerne / Universität Luzern). Sub-Units separat als unit_label modellieren (Departement/Institut). ROR-ID am besten auf Gesamtinstitution anwenden; Sub-Units via internes Mapping. Bewertung: 2 Punkte: klare Canonical-Strategie + Sub-Unit-Trennung. 1 Punkt: plausible Notes (Reorg/Sprachvarianten). M09-L2-DEDUPE-1: Dedup-Regeln: Formuliere 3 Regeln (priorisiert), um Dubletten zu erkennen (DOI, dann Identifier-ähnliche, dann Near-Dup). Lösungsrahmen: DOI exakt gleich => Dublette. (Titel normalisiert + Jahr + erster Autor) sehr ähnlich => Near-duplicate; manuelle Prüfung. Preprint/VoR: Relationen/Versionen explizit kennzeichnen statt „löschen“. Bewertung: 1 Punkt: DOI-Regel korrekt. 1 Punkt: Near-Dup-Regel mit Normalisierung. 1 Punkt: Versionierungs-Hinweis (Preprint/VoR). M09-L2-METHODBOX-1: Schreibe einen Methodik-Kasten (5–8 Sätze) für einen Departements-Report. Muss enthalten: Quelle, Stichtag, Zeitraum, Identifier-Priorität (DOI/ORCID/ROR), Dedup, Disambiguierung, Limitationen. Lösungsrahmen: Quelle+Stichtag; Zeitraum+Dokumenttypen; Identifier-Priorität (ORCID/ROR/DOI); Dedup + Disambiguierung kurz; 2–3 Limitationen (Coverage/Missingness/Unsicherheit). Bewertung: 2 Punkte: alle Pflichtfelder enthalten. 1 Punkt: Limitationen konkret (nicht generisch). Level 3 M09-L3-CASE: Mini-Case: Du sollst ein Uni-Dashboard bauen, das monatlich aktualisiert wird. Entwirf (a) einen reproduzierbaren Workflow (5–7 Schritte) und (b) eine QA-Strategie (mind. 8 Checks), die bei jedem Lauf automatisch oder halbautomatisch geprüft wird. Deliverable: Workflow + QA-Strategie (Bulletpoints). Lösungsrahmen: Workflow: Datenabruf (Query/Export) -> Snapshot speichern -> Cleaning/Mapping -> Dedup -> Disambiguierung -> Kennzahlen -> Outputs (CSV/JSON + Report) -> Versionieren/Changelog. QA: Identifier-Quoten, DOI-Duplikate, Missing affiliations, Top-Einheiten plausibilisieren, Ausreißer-Checks, Version/Parameter-Logging, Edge-Case-Set, Trend-Drift-Checks (Sprünge), Reorg-Mapping-Integrity. Bewertung: Workflow ist reproduzierbar (Snapshot/Version/Parameter) (3 Punkte). QA ist konkret und überprüfbar (nicht nur allgemein) (4 Punkte). Transparenz/Responsible-Metrics-Logik sichtbar (2 Punkte). Quellen https://docs.openalex.org/api-entities/authors https://help.openalex.org/hc/en-us/articles/24347048891543-Author-disambiguation https://docs.openalex.org/api-entities/works/work-object/authorship-object https://docs.openalex.org/additional-help/faq https://orcid.org/ https://support.orcid.org/hc/en-us/articles/360006897334-What-is-an-ORCID-iD-and-how-do-I-use-it https://ror.org/ https://www.crossref.org/documentation/retrieve-metadata/rest-api/ https://www.doi.org/ https://www.iso.org/standard/81599.html https://support.datacite.org/docs/doi-basics https://www.elsevier.com/products/scopus/author-profiles"
    },
    {
      "url": "modul10.html",
      "title": "Modul 10: Datenquellen & Coverage",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 10: Datenquellen & Coverage Modul 10: Datenquellen & Coverage Lernziele Die vier großen Quellenfamilien (WoS, Scopus, OpenAlex/Crossref, Google Scholar) nach Kurationsgrad, Coverage, Transparenz, APIs und Reproduzierbarkeit unterscheiden. Coverage-Bias (Feld/Region/Sprache/Publikationstyp) als Haupttreiber für unterschiedliche Ergebnisse erklären. Für konkrete Use-Cases eine passende Datenquelle (oder Quelle + Triangulation) auswählen und begründen. Einen „Source Disclosure“-Methodik-Kasten schreiben, der Quelle, Stichtag, Exportregeln, Limitationen und Reproduzierbarkeit abdeckt. Kurz erklärt Die Wahl der Datenquelle ist eine methodische Entscheidung – nicht nur eine Tool-Frage. WoS/Scopus sind kuratierte, kostenpflichtige Zitationsindizes mit definierten Selektionsprozessen. OpenAlex ist offen und sehr breit (primär gespeist aus Crossref und weiteren Quellen). Google Scholar ist extrem breit, aber weniger transparent und schwieriger reproduzierbar. Für seriöse Bibliometrie gilt: Quelle offenlegen, Use-Case passend wählen, und bei heiklen Aussagen triangulieren. Vergleichsmatrix (didaktisch für die Website) Dimension Web of Science (WoS Core Collection) Scopus OpenAlex (primär Crossref-basiert, offen) Crossref (Metadata Registry, offen) Google Scholar Kosten/Zugang Abo (institutionell). Abo (institutionell). Offen (API/Downloads). Offen (REST API). Kostenlos (Web). Kuration/Selektion Stark kuratiert, redaktioneller Selektionsprozess, Re-Evaluation. Kuratiert durch Content Selection and Advisory Board (CSAB). Keine Journal-Kuration im Sinne von „selektiv aufnehmen“; sehr breit, abhängig von Quellfeeds. Registry-Logik (Publisher registrieren DOIs); keine „Qualitätsselektion“ wie WoS/Scopus. Sehr breit, weitgehend automatisiert; weniger transparent. Coverage (Feld/Region/Typ) Selektiv; gute Zitierverknüpfung, je nach Collection/Feld unterschiedlich. Breit, oft stärker in Proceedings/Internationalität; dennoch kuratiert. Sehr breit; stark beeinflusst durch Crossref-Registrierung/Metadatenqualität; Typisierung kann Grenzen haben. Breit für DOI-registrierte Inhalte; Referenzen/Metadaten je Publisher stark unterschiedlich. Sehr groß (inkl. nicht-traditioneller Outputs); dadurch häufig höhere Zitationszahlen. Transparenz (Regeln/Docs) Dokumentiert (Selection/Indexing/Policies). Relativ gut (Coverage Guides). Sehr gut dokumentiert (Entities, FAQ, Quellen). Sehr gut dokumentiert (REST API, Metadata). Begrenzt; Reproduzierbarkeit und Coverage schwer exakt zu dokumentieren. APIs/Export Professionelle Exporte/Integrationen (abhängig von Lizenz). APIs/Exports abhängig von Lizenz; breit genutzt in Research Intelligence. Starke, offene API; gut für reproduzierbare Pipelines. Sehr gut (REST API). Keine offizielle offene API für systematische Analysen; Export/Harvesting eingeschränkt. Reproduzierbarkeit Gut, wenn Exportparameter und Stichtag dokumentiert werden. Gut bei sauberer Dokumentation (Stichtag, Query, Dokumenttypen). Sehr gut, wenn Snapshot/Stichtag und Query fixiert werden. Sehr gut (DOI als Key, Stichtag dokumentieren). Schwächer (Ranking/Indexing/Profiles ändern sich; Limitierungen beim systematischen Zugriff). Stärken Hohe Datenkonsistenz, kuratierte Collections, etablierte Evaluation-Workflows. Breite Abdeckung, stabile Metadaten-Pipelines, viele Metriken/Produkte (z. B. CiteScore). Open workflows, skalierbare Analysen, gute Lehr-/Demo-Umgebung, Integration mit offenen IDs. Stabiler Work-Identifier-Backbone (DOI), Basis für Metadaten-Abgleich und offene Workflows. Auffinden von grauer Literatur, lokalen/kleineren Outputs, breites Suchnetz. Risiken/Limitations Coverage-Bias (Feld/Region/Sprache), „Selektivität“ kann SSH/Local outputs benachteiligen. Coverage-Unterschiede zu WoS/anderen; Backfill/Indexing-Regeln können Ergebnisse beeinflussen. Abhängigkeit von Crossref & Metadatenqualität; Disambiguierung/Typen/Ref-Links nicht überall gleich stark wie kuratierte Zitationsindizes. Zitations-/Referenzdaten nicht vollständig/uneinheitlich; nicht „Citation index“ im WoS/Scopus-Sinn. Intransparenz, Dubletten/Fehlzuordnungen, schwierig auditierbar für institutionelles Reporting. Typische Use-Cases Formale Evaluation (mit Responsible-Metrics-Leitplanken), Zitationsanalysen, standardisierte Reports. Forschungsmonitoring, Portfolios, Feld-/Themenanalysen, Kombination mit SciVal-Workflows. Lehre, Prototyping, offene Dashboards, Triangulation, erste Feld-/Netzwerkanalysen. Metadaten-Enrichment, Dedup/QA, DOI-basierte Datensätze, offene Pipelines. Literatursuche, ergänzende Triangulation, individuelle Profile (mit QA), frühe Orientierung in Nischen. Entscheidungshilfe nach Use-Case (praktisch) Offizielles Uni-Reporting / evaluative Berichte: Primär WoS oder Scopus (je nach Lizenz/Tradition), plus klare Responsible-Metrics-Policy und Transparenzbox; OpenAlex als Triangulation/QA möglich. Lehre & Training (Bibliometrie-Einführung, Übungen, Reproduzierbarkeit): OpenAlex + Crossref (offen, API, reproduzierbar). Ergänzend: Demo-Screenshots aus WoS/Scopus (wenn vorhanden) zur Diskussion von Kuration vs. Coverage. Open Science / offene Dashboards / Prototyping: OpenAlex/Crossref als Backbone; optional DataCite (für Daten/Software) – mit klarer Datenqualitätssektion. SSH / lokale Sprachen / graue Literatur / Buchfokus: Triangulation: Google Scholar (Discovery) + kuratierte Quellen (falls relevant) + offene Metadaten (Crossref/OpenAlex). Aussagekraft immer mit Coverage-Hinweisen begrenzen. Affiliation- und Organisationsanalysen: Quelle mit guter Affiliation-Qualität wählen; immer ORG-Disambiguierung/Mappings (ROR) + QA (Top-Units, Dubletten). Source Disclosure Box (Copy/Paste für Website & Reports) Datenquelle(n): | Stichtag: | Query/Export: | Zeitraum: | Dokumenttypen: | Zitationsfenster: | Counting: | Disambiguierung: | Coverage-Hinweis: | Reproduzierbarkeit: . Typische Fehler bei Source-Vergleichen „Die Datenbank mit mehr Records ist automatisch besser.“ Mehr Coverage kann mehr Rauschen und weniger Auditierbarkeit bedeuten. „Besser“ hängt vom Use-Case ab. „Zitationszahlen sind zwischen Quellen direkt vergleichbar.“ Coverage, Referenzverknüpfung, Dokumenttypen und Indexing-Regeln unterscheiden sich – daher sind Zahlen nicht 1:1 austauschbar. „Google Scholar ist für institutionelle Bibliometrie genauso auditierbar wie WoS/Scopus.“ Ohne stabile Export-/API-Workflows und klare Coverage-Regeln ist reproduzierbares, prüfbares Reporting schwieriger. Responsible-Metrics-Box Quelle ist Teil der Methodik: immer offenlegen (inkl. Stichtag/Exportregeln). Bei heiklen Aussagen triangulieren (mindestens zweite Quelle / Plausibilisierung). Coverage-Bias sichtbar machen (Feld/Region/Sprache/Publikationstyp). Reproduzierbarkeit priorisieren: Snapshots/Parameter/Versionierung. Mini-Übungen (Level 1–3) Level 1: Welche Kombination passt am besten? WoS = offen und unkuratierte Universalindexierung OpenAlex = offen, primär aus Crossref gespeist, keine selektive Journal-Kuration Google Scholar = stark kuratiert durch in-house Editors Warum unterscheiden sich Zitationszahlen zwischen Datenquellen am häufigsten? Weil Mathematik falsch ist. Weil Coverage/Indexing/Regeln (Dokumenttypen, Referenzen) unterschiedlich sind. Weil nur eine Quelle echte Zitationen zählt. Level 2: Du baust einen Lehrgangs-Datensatz für Übungen ohne Paywall. Welche Quelle(n) wählst du und wie begründest du das in 3 Bulletpoints? Triangulation: Du findest 20 % mehr Publikationen in Source A als in Source B für dasselbe Departement. Nenne 4 plausible Gründe. Level 3: Mini-Case: Die Uni will ein jährliches Forschungsdashboard. Entwirf (a) eine Source-Strategie (1–2 Hauptquellen + Triangulation) und (b) eine Source Disclosure Box (vollständig) für die Website. Quellen & weiterführende Links https://clarivate.com/academia-government/scientific-and-academic-research/research-discovery-and-referencing/web-of-science/web-of-science-core-collection/content-collection-and-indexing-process/ https://clarivate.com/academia-government/scientific-and-academic-research/research-discovery-and-referencing/web-of-science/web-of-science-core-collection/editorial-selection-process/ https://assets.ctfassets.net/o78em1y1w4i4/EX1iy8VxBeQKf8aN2XzOp/c36f79db25484cb38a5972ad9a5472ec/Scopus_ContentCoverage_Guide_WEB.pdf https://docs.openalex.org/additional-help/faq https://docs.openalex.org/api-entities/sources https://www.crossref.org/documentation/retrieve-metadata/rest-api/ https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1729 https://blogs.lse.ac.uk/impactofsocialsciences/2019/12/03/google-scholar-web-of-science-and-scopus-which-is-best-for-me/"
    },
    {
      "url": "modul11.html",
      "title": "Modul 11: Science Mapping & Wissensstrukturen: Co-Citation, Bibliographic Coupling, Co-Word (inkl. VOSviewer/bibliometrix)",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 11: Science Mapping & Wissensstrukturen: Co-Citation, Bibliographic Coupling, Co-Word (inkl. VOSviewer/bibliometrix) Modul 11: Science Mapping & Wissensstrukturen: Co-Citation, Bibliographic Coupling, Co-Word (inkl. VOSviewer/bibliometrix) Lernziele Die drei klassischen Science-Mapping-Ansätze (co-citation, bibliographic coupling, co-word) definieren und für passende Fragestellungen auswählen. Verstehen, warum co-citation eher die \"intellektuelle Basis\" (Vergangenheit) und bibliographic coupling eher die \"Forschungsfront\" (Gegenwart) sichtbar macht. Grundbegriffe von Netzwerken (Knoten, Kanten, Gewicht, Normalisierung, Thresholds, Cluster) korrekt anwenden. Ein Mapping-Projekt reproduzierbar durchführen: Datenbasis, Parameter, Cleaning, Thresholds, Visualisierung, Interpretation. Typische Fehlinterpretationen (Cluster = \"Schulen\", Farben = \"Qualität\", Kanten = \"Kausalität\") vermeiden. Kurz erklärt Science Mapping beschreibt Forschungsfelder als Netzwerke. Du kannst damit z. B. die intellektuellen Grundlagen (co-citation), die aktuelle Forschungsfront (bibliographic coupling) oder Themenlandschaften (co-word/co-occurrence) sichtbar machen. Die Qualität steht und fällt mit Datenbasis, Cleaning (Orgs/Keywords), Thresholds und transparenter Dokumentation. Schlüsselbegriffe Co-citation: Zwei Dokumente gelten als co-cited, wenn sie gemeinsam von anderen Dokumenten zitiert werden; Stärke = Häufigkeit des gemeinsamen Zitierens. Bibliographic coupling: Zwei Dokumente sind gekoppelt, wenn sie gemeinsame Referenzen haben; Stärke = Anzahl geteilter Referenzen. Co-word analysis / Co-occurrence: Analyse von gemeinsam auftretenden Begriffen/Keywords (oder Terms im Abstract/Title) zur Themen- und Strukturkartierung. Node (Knoten): Ein Element im Netzwerk (z. B. Paper, Autor:in, Journal, Keyword, Organisation). Edge (Kante): Verbindung zwischen zwei Knoten (z. B. co-citation link, coupling link, co-occurrence link). Weight (Gewicht): Stärke einer Kante (z. B. #co-citations, #shared references, #co-occurrences). Normalization: Skalierung/Anpassung der Gewichte, damit große Knoten/hochfrequente Items nicht alles dominieren. Threshold: Mindestkriterium, ab wann ein Item/Kante in die Karte aufgenommen wird (z. B. min. 20 Zitationen). Clustering: Algorithmische Gruppierung von Items in Cluster (dichte Teilnetze), oft als Themen-/Schul-Nähe interpretiert – aber nicht 1:1 identisch. Overlay visualization: Darstellung eines Zusatzmerkmals (z. B. mittleres Jahr, FWCI) als Farb-/Skalenlayer – interpretieren mit Vorsicht. Drei Ansätze – wann welcher passt 1) Co-citation (Wissensbasis / \"intellectual base\") Nutze co-citation, wenn du verstehen willst, *welche klassischen Werke/Theorien* ein Feld zusammenhalten. 2) Bibliographic coupling (Forschungsfront / \"research front\") Nutze coupling, wenn du sehen willst, *welche aktuellen Publikationen* auf ähnlichen Referenzfundamenten stehen (oft zeitnäher als co-citation). 3) Co-word / Co-occurrence (Themen & Begriffe) Nutze co-word, wenn du Themencluster, Begriffsräume oder Trendbegriffe kartieren willst (Keywords oder Terms aus Titles/Abstracts). Interpretation: Was eine Karte zeigt (und was nicht) Eine Karte zeigt Ähnlichkeit/Relatedness nach dem gewählten Link-Typ. Nähe bedeutet: „stärker verbunden“ – nicht automatisch „inhaltlich identisch“, nicht „besser“, nicht „kausal“. Cluster sind algorithmische Gruppen; ihre Benennung ist ein interpretativer Schritt (immer mit Stichprobenprüfung der Top-Items pro Cluster). Typische Fehlinterpretationen „Ein Cluster ist eine objektive 'Schule'.“ Cluster hängen von Datenquelle, Thresholds, Normalisierung und Algorithmusparametern ab. „Mehr Kanten = bessere Forschung.“ Netzwerkdichte misst Verbundenheit/Kommunikation, nicht Qualität. „Farben im Overlay zeigen Impact/Qualität.“ Overlay zeigt das gewählte Merkmal (z. B. Jahr) und ist anfällig für Ausreißer/Selection-Bias. „Die Karte ist 'die Wahrheit' über das Feld.“ Karten sind Modelle; seriöse Nutzung braucht Triangulation + methodische Transparenz. Praxis-Workflow: Science Mapping reproduzierbar Fragestellung + Einheit definieren (Papers, Journals, Autor:innen, Keywords). Datenquelle wählen (WoS/Scopus/OpenAlex) + Exportparameter fixieren (Zeitraum, Doc types, Query). Cleaning: Dubletten entfernen; Orgs/Keywords vereinheitlichen (Thesaurus/Mappings). Link-Typ wählen: co-citation vs coupling vs co-word (passend zur Frage). Thresholds setzen (min. cites/occurrences) + Sensitivitätscheck (2–3 Varianten). Normalisierung/Counting dokumentieren (Tool-defaults sind Teil der Methodik!). Clustering + Labeling: pro Cluster Top-Items prüfen und beschriften. Output: Karte + Tabellen (Top-Items/Top-links) + Methodik-Kasten + Limitationen. Mini-Lab: VOSviewer Karte (30–45 Minuten) Export aus WoS/Scopus (oder OpenAlex->CSV) mit Referenzen/Keywords. VOSviewer: Create map -> Bibliographic data. Wähle Map-Typ: Co-citation (Cited References oder Sources) ODER Bibliographic coupling (Documents/Sources) ODER Co-occurrence (All keywords/Terms). Setze Thresholds so, dass ~100–400 Items übrig bleiben (lesbar, aber aussagekräftig). Nutze Thesaurus-Datei für Synonyme/Varianten (Keywords/Orgs). Interpretation: Pro Cluster 10 Top-Items prüfen; Cluster benennen; 3 zentrale Knoten erklären. Mini-Lab (optional): bibliometrix (R) als reproduzierbare Pipeline Import der Exportdatei (WoS/Scopus) in bibliometrix. Erstelle (a) co-citation, (b) coupling, (c) co-word Netzwerke mit identischen Thresholds. Vergleiche: Welche Cluster bleiben stabil? Wo ändern sich Grenzen? Exportiere Results als CSV/JSON (Top-Items, Cluster assignments) für die Website. Responsible-Metrics-Box Karten sind explorativ: keine Ranking- oder Qualitätsautomatik. Immer offenlegen: Datenquelle, Query, Zeitraum, Doc types, Thresholds, Normalisierung, Tool-Version. Cluster-Benennung ist interpretativ: mit Stichprobenprüfung begründen. Bei Stakeholder-Reports: Karten mit Tabellen + Methodikbox + Limitationen kombinieren. Quellen & weiterführende Links https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.4630240406 https://onlinelibrary.wiley.com/doi/10.1002/asi.5090140103 https://journals.sagepub.com/doi/10.1177/053901883022002003 https://link.springer.com/article/10.1007/s11192-009-0146-3 https://www.vosviewer.com/documentation/Manual_VOSviewer_1.6.18.pdf https://www.sciencedirect.com/science/article/pii/S1751157717300500 https://arxiv.org/abs/1301.4655"
    },
    {
      "url": "modul12.html",
      "title": "Modul 12: Forschungsbewertung in der Praxis: Reporting-Patterns, Dashboards, Use-Cases (Unit/Faculty/Person) + Do/Don’t-Katalog",
      "category": "Module",
      "content": "Fortschritt: 0% 0% Modul 12: Forschungsbewertung in der Praxis: Reporting-Patterns, Dashboards, Use-Cases (Unit/Faculty/Person) + Do/Don’t-Katalog Modul 12: Forschungsbewertung in der Praxis: Reporting-Patterns, Dashboards, Use-Cases (Unit/Faculty/Person) + Do/Don’t-Katalog Lernziele Zwischen Monitoring, Learning/Improvement und High-stakes Evaluation unterscheiden (und passende Methoden wählen). Responsible-Metrics-Leitplanken (DORA/Leiden/Metric Tide/CoARA/HKP) in konkrete Reporting- und Dashboard-Designs übersetzen. Dashboards so spezifizieren, dass sie transparent, auditierbar und fair sind (Quelle, Stichtag, Coverage, Counting, Unsicherheit). Typische Fehlanreize und Governance-Risiken erkennen und vermeiden (Gaming, Goodhart, Field-bias, Ranking-Falle). Für drei Ebenen (Institution/Unit, Faculty/Group, Person) passende Indikator-Sets und Narrative-Elemente entwerfen. Kurz erklärt Dashboards und Reports sind nie neutral: Sie formen Entscheidungen. Gute bibliometrische Praxis beginnt daher mit dem Use-Case (Monitoring vs. High-stakes), arbeitet mit einem kleinen, ausgewogenen Indikatorenset, ergänzt qualitative Beurteilung (Peer Review/Narrative Evidence) und dokumentiert die Methodik vollständig (Quelle, Stichtag, Coverage, Counting, Limitationen). Schlüsselbegriffe Low-stakes monitoring: Monitoring zur Orientierung/Verbesserung (z. B. Portfolio-Entwicklung, OA-Anteile), ohne direkte Konsequenzen für Einzelpersonen. High-stakes assessment: Bewertung mit direkten Konsequenzen (Hiring/Promotion, Mittelzuweisung, Ranking-Entscheide). Erfordert besonders strenge Leitplanken. Responsible metrics: Rahmen, der Metriken als unterstützende Evidenz versteht und Transparenz, Kontext, Fairness, Expertise und Reproduzierbarkeit verlangt. Goodhart’s law (in der Praxis): Wenn ein Indikator zum Ziel wird, verliert er Informationsgehalt (Gaming/Optimierung auf Zahl statt Qualität). Source disclosure / Methodikbox: Standardisierte Offenlegung von Datenquelle, Stichtag, Query/Exportregeln, Counting, Limitationen und Reproduzierbarkeit. Narrative CV: Narratives Format (z. B. UKRI R4RI), um Beiträge, Rollen, Open Science, Team Science, Mentoring etc. sichtbar zu machen – als Ergänzung zu Zahlen. Assessment-Spektrum: Wähle das passende Instrument A — Monitoring & Learning (low-stakes) Typische Entscheidungen: Portfolio-Entwicklung, OA-Strategie, Kooperationsanalyse, Feld-/Themenmonitoring. Empfohlene Evidenz: Zeitreihen, feld-normalisierte Indikatoren auf Aggregat, Open Science/Output-Diversität, qualitative Kontextnotizen. Strenge: mittel. B — Priorisierung & Ressourcendialog (medium-stakes) Typische Entscheidungen: Schwerpunktsetzung, Ressourcenargumentation, Programm-Review. Empfohlene Evidenz: Triangulation mehrerer Quellen, Benchmarks innerhalb Fachgruppen, Stichproben-QA + Audit-Trail, kurze Peer-Review-/Expert:innen-Notizen. Strenge: hoch. C — Hiring/Promotion/Funding (high-stakes) Typische Entscheidungen: Berufungen, Tenure/Promotion, Projektförderung. Empfohlene Evidenz: primär qualitative Beurteilung (Peer Review), Narrative CV + Evidenz, kontextualisierte, sparsame Metriken (nicht mechanisch), strikte Fairness-/Bias-Checks. Strenge: maximal. Reporting-Patterns: 5 bewährte Muster Deskriptives Profil (What we do): Transparente Bestandsaufnahme ohne Rankingdruck. Widgets: Output-Mix (Artikel/Bücher/Datasets/Software), OA-Anteil, Collaboration (int./nat.), Top-Felder/Keywords (mapping-light). Kontextualisierte Benchmark (How we compare, responsibly): Vergleich nur innerhalb sinnvoller Fachgruppen und mit Coverage-Hinweisen. Widgets: Field-normalized impact (Aggregat), Top-x% Indikatoren (Aggregat), Produktivität pro FTE (nur wenn valide), Confidence-/Missingness-Box. Diagnose & QA (Why numbers differ): Fehlerquellen sichtbar machen (Affiliations, Dubletten, Coverage). Widgets: DOI/ORCID/ROR-Quote, Missing affiliations, Dublettenzahl, Quelle-Vergleich (Triangulation). Impact Storyboard (Evidence, not vibes): Wirkung als belegte Story: Policy/Uptake/Use in Praxis + qualitative Evidenz. Widgets: Policy mentions (mit Kontext), Usage (COUNTER), Selected case studies, Narrative evidence + Links. Improvement Loop (Measure → Learn → Adjust): Indikatoren als Feedback für Prozesse (Open Science, Data sharing, reproducibility). Widgets: OS-Praktiken (OA, Data availability), Narrative contributions (Mentoring, Team Science), jährliche Indikator-Review-Notiz. Dashboard-Blueprint (minimal, aber robust) Header / Disclosure Datenquelle(n) + Stichtag + Query/Exportparameter. Zeitraum + Doc types + Counting (full/fractional). Coverage-Hinweis + Missingness/Unsicherheit. Kontakt/Feedback-Kanal (Korrekturen möglich). KPI-Kern (max. 8 KPIs) Output-Mix (divers). OA-Anteil. Field-normalized citation indicator (nur Aggregat). Top-x% Indikator (nur Aggregat). Kooperationsindikator. Policy/Practice uptake (qualifiziert, nicht nur Zahl). Usage (COUNTER, wo verfügbar). Data quality: DOI/ORCID/ROR-Quote. Drill-down (mit Leitplanken) Keine Einzelpersonen-Rankings im Default-View. Drill-down nur mit Kontext (Feld, Zeitraum, Rolle, Output-Typ). Jede Tabelle exportierbar + mit Methodik-Hinweisen. Do / Don’t-Katalog (für Website & Schulung) Do Use-case zuerst, Indikatoren danach (Monitoring ≠ Evaluation). Immer Methodikbox + Stichtag + Quelle(n) offenlegen. Feldkontext nutzen (Vergleiche nur innerhalb sinnvoller Gruppen). Kleine, ausgewogene Indikator-Sets + qualitative Ergänzung. QA sichtbar machen (Missingness, Dubletten, Disambiguierung). Regelmäßige Indikator-Reviews (weiterentwickeln, nicht versteinern). Don’t Mechanische Schwellen („JIF>…“, „h-index>…“) als harte Kriterien. Personen/Artikel mit Journalmetriken bewerten. Dashboards als Ranglistenmaschinen bauen (Goodhart/Gaming). Quellen mischen ohne Triangulation/Erklärung (Coverage-Fallen). Zahlen ohne Unsicherheit/Missingness präsentieren. „Impact“ behaupten ohne Evidenz (Fallbeispiele, Uptake-Nachweise). Governance-Check: Wer entscheidet was? Policy: Welche Entscheidungen sind high-stakes? (=> strengste Leitplanken) Roles: Wer ist Data Steward? Wer verantwortet Methodik/QA? Appeals: Korrekturweg für falsch zugeordnete Outputs/Affiliations. Transparency: Veröffentlichung von Methodik & Versionen. Fairness: Feld-/Karrierephase-/Rollen-Dimensionen explizit berücksichtigen. Responsible Metrics – Kurzrahmen (kombiniert, praxistauglich) Metriken unterstützen qualitative Beurteilung (Leiden). Klar, transparent, spezifisch, kontextualisiert, fair (DORA-Guidance). Reformpfad: Vielfalt der Beiträge anerkennen, qualitative Urteile zentral (CoARA). Integrität/Transparenz/Open Science belohnen (Hong Kong Principles/UNESCO OS). Framework „Responsible Metrics“ operationalisieren (Metric Tide). Übungen (Level 1–3) Level 1: High-stakes-Aussage identifizieren und die 5 DORA-Prinzipien benennen. Level 2: Dashboard-Spezifikation (max. 12 Bulletpoints) für ein Institut schreiben; kritische Review eines Rankings (5 Probleme + 3 Verbesserungen). Level 3: Bewertungs-Set für eine Berufungskommission entwerfen (qualitative Kriterien, kontextualisierte Metriken, Narrative Template). Quellen & weiterführende Links https://www.ukri.org/wp-content/uploads/2021/12/RE-151221-TheMetricTideFullReport2015.pdf https://sfdora.org/wp-content/uploads/2024/05/DORA_indicators_guidance.pdf https://www.nature.com/articles/520429a https://www.coara.org/wp-content/uploads/2025/11/2022_07_19_rra_agreement_final.pdf-3.pdf https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000737 https://www.ukri.org/apply-for-funding/develop-your-application/resume-for-research-and-innovation-r4ri-guidance/ https://www.unesco.it/wp-content/uploads/2023/11/RECOMMENDATION-ON-OPEN-SCIENCE-2021-Certified.pdf"
    }
  ]
}
